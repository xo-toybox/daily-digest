{
  "url": "https://arxiv.org/html/2411.15594v6",
  "type": "webpage",
  "content": "    A Survey on LLM-as-a-Judge           \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n    1 1 footnotetext: These authors contributed equally to this research. 2 2 footnotetext: Corresponding author.  A Survey on LLM-as-a-Judge \n    Jiawei Gu 1,2* , Xuhui Jiang 1,3* , Zhichao Shi 1,4,* , Hexiang Tan 4 , Xuehao Zhai 5 , Chengjin Xu 1,3 , Wei Li 4 , Yinghan Shen 4 , Shengjie Ma 1,6 , Honghao Liu 1 , \n Saizhuo Wang 1,7 ,Kun Zhang 4 , Zhouchi Lin 1 , Bowen Zhang 1 , Lionel Ni 7,8 , Wen Gao 9 , Yuanzhuo Wang 4,\u2020 , Jian Guo 1,\u2020    \n 1 IDEA Research, International Digital Economy Academy China  \n 2 Sun Yat-sen University China  \n 3 DataArc Tech Ltd China  \n 4 Institute of Computing Technology, Chinese Academy of Sciences China  \n 5 Department of Civil and Environmental Engineering, Imperial College London UK  \n 6 Gaoling School of Artificial Intelligence, Renmin University of China \n 7 The Hong Kong University of Science and Technology China  \n 8 The Hong Kong University of Science and Technology (Guangzhou) China  \n 9 Department of Computer Science and Technology, Peking University China   \n   Abstract. \n   Abstract \n   Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale.\nLarge Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of \u201dLLM-as-a-Judge,\u201d where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable and flexible assessments, LLMs present a compelling alternative to traditional expert-driven evaluations.\nHowever, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization.\nThis paper provides a comprehensive survey on LLM-as-a-Judge, offering a formal definition and a detailed classification , while focusing on addressing the core question: How to built reliable LLM-as-a-Judge systems? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose.\nTo advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. \n  \n   This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field. Our contributions span multiple levels: we establish the conceptual boundaries of LLM-as-a-Judge, reorganize fragmented literature into a unified framework, and propose a novel reliability-oriented benchmark. Building on these, we also articulate a forward-looking research agenda, offering both theoretical foundations and practical guidance for constructing reliable and socially trustworthy LLM-as-a-Judge systems.\nThe associated resources can be accessed at https://awesome-llm-as-a-judge.github.io/ . \n  \n   \n  \u2020 \u2020 copyright: none  \n    1. Introduction \n    Judgment is the faculty of thinking the particular as contained under the universal.\nIt involves the capacity to subsume under rules, that is, to distinguish whether something falls under a given rule. \n  \u2014\u2014 Kant, Critique of Judgment  (Kant, 1790 ) , Introduction IV, 5:179 ; Critique of Pure Reason  (Kant, 1781 ) , A132/B171 . \n   \n   Recently, Large Language Models (LLMs) have achieved remarkable success across numerous domains (Xu et\u00a0al., 2024b ) , ranging from technical fields (Zhao et\u00a0al., [n.\u2009d.] ; Tang et\u00a0al., 2024b ; Yuan et\u00a0al., 2023a ) to the humanities (Qu et\u00a0al., 2024 ; Luo et\u00a0al., 2024 ; Zhong et\u00a0al., 2023 ; Jiang et\u00a0al., 2023 ) and social sciences (Wang et\u00a0al., 2025b ; Xu et\u00a0al., 2023a ; He et\u00a0al., 2023 ; Shi, 2024 ) .\nThis growing interest stems from LLMs\u2019 ability to mimic human-like reasoning and thinking processes, enabling them to take on roles traditionally reserved for human experts while offering a cost-effective solution that can be effortlessly scaled to meet increasing evaluation demands.\nFor instance, the use of LLM-as-a-Judge in academic peer review 1 1 1 https://blog.iclr.cc/2024/10/09/iclr2025-assisting-reviewers/ offers a potential means to address the sharp growth in submissions while sustaining expert-level judgments. \n  \n   Before the era of LLMs, finding a balance between comprehensive and scalable evaluation posed a persistent challenge. On the one hand, widely used subjective methods like expert-driven assessments (Shi et\u00a0al., 2024 ; Gao et\u00a0al., 2023b ) integrate holistic reasoning and fine-grained contextual understanding, making them the gold standard in comprehensiveness. However, these approaches are costly, difficult to scale, and susceptible to inconsistency.\nOn the other hand, objective assessment methods, such as automatic metrics, offer strong scalability and consistency. For example, tools such as BLEU (Papineni et\u00a0al., 2002 ) or ROUGE (Lin, 2004 ) can rapidly evaluate machine-generated translations or summaries against reference texts without human intervention. However, these metrics, which heavily rely on surface-level lexical overlaps, often fail to capture deeper nuances, resulting in poor performance in tasks like story generation or instructional texts (Schluter, 2017 ) .\nAs a solution to this persistent dilemma, \u201cLLM-as-a-Judge\u201d has emerged as a promising idea to combine the strengths of the above two evaluation methods. Recent studies have shown that this idea can merge the scalability of automatic methods with the detailed, context-sensitive reasoning found in expert judgments (Zheng et\u00a0al., 2023b ; Wang et\u00a0al., 2023d ; Zhu et\u00a0al., 2023 ; Li et\u00a0al., 2023b ; Chen et\u00a0al., 2024c ) .\nMoreover, LLMs may become sufficiently flexible to handle multimodal inputs (Chen et\u00a0al., 2024b ) under appropriate prompt learning or fine-tuning (Khattak et\u00a0al., 2023 ) .\nThese advantages suggest that the LLM-as-a-Judge approach could serve as a novel and broadly applicable paradigm for addressing complex and open-ended evaluation problems. \n  \n   LLM-as-a-Judge holds significant potential as a scalable and adaptable evaluation framework compared to the aforementioned two traditional methods (Wang et\u00a0al., 2024b ) . However, its widespread adoption is hindered by two key challenges. The first challenge lies in the absence of a systematic review, which highlights the lack of formal definitions, fragmented understanding, and inconsistent usage practices in the relevant studies. As a result, researchers and practitioners struggle to fully understand and apply effectively. The second challenge concerns reliability (Yu et\u00a0al., 2024b ) , as merely employing LLM-as-a-Judge does not ensure accurate evaluations aligned with established standards.\nThese challenges emphasize the need for a deeper assessment of the outputs generated by LLM-as-a-Judge, as well as a crucial investigation into the question: How to build reliable LLM-as-a-Judge systems? \n  \n   To address these challenges, this paper provides a systematic review of research on LLM-as-a-Judge. It offers a comprehensive overview of the field and explores strategies for building reliable LLM-as-a-Judge systems. We begin by defining LLM-as-a-Judge through both formal and informal definitions, answering the foundational question: \u201dWhat is LLM-as-a-Judge?\u201d Next, we categorize existing methods and approaches, exploring \u201dHow to use LLM-as-a-Judge?\u201d .\nFollowing this, to tackle the critical question: \u201dHow to build reliable LLM-as-a-Judge systems?\u201d , we explore two core aspects: (1) strategies to enhance the reliability of LLM-as-a-Judge systems and (2) methodologies for evaluating the reliability of these systems.\nFor the first aspect, we review key strategies to optimize the performance of LLM-as-a-Judge. For the second aspect, we examine the metrics, datasets, and methodologies used to evaluate LLM-as-a-Judge systems, highlighting potential sources of bias and methods for their mitigation.\nBuilding on this, we introduce a novel benchmark specifically designed for evaluating LLM-as-a-Judge systems. Finally, we discuss future research directions, emphasizing key areas for improving reliability, scalability, and applicability. The contributions of this study can be summarized as follows: \n    (1)   At the definitional level , we establish both formal and informal definitions of LLM-as-a-Judge, thereby delineating the conceptual boundaries of this emerging paradigm. We also introduce a contextualized definition of reliability, which incorporates input variability, model characteristics, and contextual dependencies, providing a principled foundation for theorizing and building reliable systems. \n  \n  \n   (2)   At the framework level , we conduct a systematic reorganization of fragmented literature into a unified conceptual structure. Specifically, we map prior work to four foundational questions: what it is, how to use it, how to improve it, and how to evaluate it\u2014framing reliability as the unifying thread across these dimensions. \n  \n  \n   (3)   At the empirical level , we perform comparative analyses of existing approaches and further propose a meta-evaluation benchmark specifically tailored for evaluating LLM-as-a-Judge systems. This benchmark facilitates systematic reliability assessment, uncovering key trade-offs such as robustness versus sensitivity, and offering actionable insights for constructing trustworthy evaluation frameworks. \n  \n  \n   (4)   At the perspective level , we offer a comprehensive analysis that integrates the applications, challenges, and future directions of LLM-as-a-Judge, providing a roadmap that extends beyond the scope of existing surveys. By systematically reviewing its applications in core machine learning and high-stakes domains, we identify domain-specific reliability requirements and underexplored challenges such as meta-evaluation and long-term consistency. Building on these findings, we articulate a forward-looking agenda that emphasizes theoretically grounded methodologies, systematic benchmarks, and hybrid human\u2013AI frameworks for constructing reliable and socially trustworthy systems. \n  \n  \n   \n   The rest of this survey is organized as Figure 1 . Specifically, Section 2 provides an overview of the LLM-as-a-Judge field, including its definitions and categorization of existing methods. For a quick guide on the implementation of an LLM as a judge for specific scenarios, you can find answers in Quick Practice\u00a0( 2.5 ). Strategies for enhancing and evaluating the reliability of LLM-as-a-Judge systems are discussed in Sections 3 and 4 respectively. Notably, in Section 7.1 , we discuss the synergy between LLM-as-a-Judge and Reasoning-Centric enhancement, where dynamic feedback is used to optimize reasoning paths and significantly improve the model\u2019s ability to solve complex problems. Section 5 explores practical applications, while Sections 6 and 7 address challenges and outline future research directions. Finally, Section 8 presents our conclusions. \n  \n   Figure 1 . The overall framework of this paper.   \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n     2. Background and Method \n   The capacity of LLMs to emulate human reasoning and evaluate specific inputs against a set of predefined rules has paved the way for \u201dLLM-as-a-Judge.\u201d Existing studies indicate that LLM\u2019s scalability, adaptability, and cost-effectiveness make them well-suited for a growing number of evaluative tasks that were traditionally done by humans. These abilities are key in utilizing LLMs flexibly across various evaluation scenarios and objectives. As a result, the adoption of LLM in evaluation has progressed rapidly in practice. Initially, the primary focus of LLMs was on language generation and comprehension. With advancements in training paradigms like Reinforcement Learning from Human Feedback (RLHF) (Ouyang et\u00a0al., 2022 ) , LLMs became increasingly aligned with human values and reasoning processes. This alignment has allowed LLMs to transition from generative tasks to evaluation. At its core, LLM-as-a-Judge denotes the use of LLMs to evaluate objects, actions, or decisions based on predefined rules, criteria, or preferences. It encompasses a broad spectrum of roles, including: Graders  (Trung et\u00a0al., 2024 ; Dong et\u00a0al., 2023 ) , Evaluators/Assessors  (Li et\u00a0al., 2024b ; Zhang et\u00a0al., 2024d ) , Critics  (Ke et\u00a0al., 2024 ; Xiong et\u00a0al., 2024 ; Putta et\u00a0al., 2024 ) , Verifiers  (Ling et\u00a0al., 2024 ; Shinn et\u00a0al., 2023 ; Wang et\u00a0al., 2024e ) , Examiners  (Bai et\u00a0al., 2023 ) , Reward/Ranking Models  (Yang et\u00a0al., 2024b ; Sun et\u00a0al., 2023a ; Luo et\u00a0al., 2023a ; Yuan et\u00a0al., 2023b ) , etc. \n  \n   Currently, the definition of how to effectively use LLM-as-a-Judge for evaluation tasks is largely informal or vague, lacking a clear and formal expression. Therefore, we will start with a formal definition of LLM-as-a-Judge as follows: \n  \n       \u2130 \\displaystyle\\mathcal{E} \u2190 \\displaystyle\\leftarrow \ud835\udcab \u2112 \u200b \u2112 \u200b \u2133 \\displaystyle\\mathcal{P}_{\\mathcal{LLM}} ( x \u2295 \ud835\udc9e ) \\displaystyle\\left(x\\oplus\\mathcal{C}\\right)      \n     \u2022   \u2130 \\mathcal{E} : The final evaluation obtained from the whole LLM-as-a-Judge process in the expected manner. It could be a score, a choice, a label or a sentence, etc. \n  \n  \n   \u2022   \ud835\udcab \u2112 \u200b \u2112 \u200b \u2133 \\mathcal{P}_{\\mathcal{LLM}} : The probability function defined by the corresponding LLM, and the generation is an auto-regressive process. \n  \n  \n   \u2022   x x : The input data in any available types\u00a0(text, image, video), which waiting to be evaluated. \n  \n  \n   \u2022   \ud835\udc9e \\mathcal{C} : The context for the input x x , which is often prompt template or combined with history information in dialogue. \n  \n  \n   \u2022   \u2295 \\oplus : The combination operator combines the input x x with the context \ud835\udc9e \\mathcal{C} , and this operation can vary depending on the context, such as being placed at the beginning, middle, or end. \n  \n  \n   \n   The formulation of LLM-as-a-Judge reflects that LLM is a type of auto-regressive generative model, which generates subsequent content based on the context to obtain target evaluation. It illustrates how we utilize LLM for evaluation tasks, encompassing input design, model selection, and training, as well as output post-processing.\nThe basic approaches of implementing LLM-as-a-Judge can be classified by the formulation: In-Context Learning, Model Selection, Post-processing Method, and Evaluation Pipeline in Figure 2 . By following this pipeline, one can build a basic LLM-as-a-Judge for evaluation. A quick practice guide is available in section 2.5 .\nHowever, the basic definition alone does not guarantee the reliability of evaluations. To explicitly highlight and address reliability, we further propose the following enhanced formal definition: \n     \u211b \u2190 f R \u200b ( \ud835\udcab \u2112 \u200b \u2112 \u200b \u2133 , x , \ud835\udc9e ) \\Large\\mathcal{R}\\leftarrow f_{\\text{R}}\\left(\\mathcal{P}_{\\mathcal{LLM}},x,\\mathcal{C}\\right)     \n     \u2022   \u211b \\mathcal{R}_{\\text{}} : The evaluation explicitly designed to ensure consistency, robustness, and alignment with human judgment. This reliability is verified through additional validation, calibration, and standardization steps beyond the basic pipeline. \n  \n  \n   \u2022   f R f_{\\text{R}} : A series of constraints and validation methods applied systematically to the basic LLM-as-a-Judge framework to enhance evaluation reliability. These include methods to mitigate biases, control variability, and confirm robustness against adversarial inputs. \n  \n  \n   \n   Figure 2 . LLM-as-a-Judge evaluation pipelines.     2.1. In-Context Learning  \n   To apply LLM-as-a-Judge, evaluation tasks are typically specified using In-Context Learning methods, which provide instructions and examples to guide the model\u2019s reasoning and judgment. This process involves two key aspects: input design and prompt design. For input design, it is important to consider the type of variables to be evaluated (such as text, image, or video), the manner of input (e.g., individually, in pairs, or in batches), and its position (e.g., at the beginning, middle, or end). For the prompt design, four different methods can be adopted, as illustrated in Figure 2 . These methods include generating scores, solving true/false questions, conducting pairwise comparisons, and making multiple-choice selections. Further details will be presented in the following sections. \n  \n    2.1.1. Generating scores  \n   It is quite intuitive to represent an evaluation using a corresponding score, shown in Figure 3 . What requires more careful consideration, however, is the nature and range of the score used for evaluation. The score can be discrete, with common ranges like 1-3, 1-5 (Jones et\u00a0al., 2024 ) , or 1-10 (Zhu et\u00a0al., 2023 ; Li et\u00a0al., 2023b ) . Alternatively, it can be continuous, ranging from 0 to 1 or 0 to 100 (Xiong et\u00a0al., 2024 ) .\nThe simplest way to score is through the context, setting the range of scores and the main criteria for scoring. For example, \u201dPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance\u201d (Zhu et\u00a0al., 2023 ) . A slightly more complex way is to provide more detailed scoring criteria. More complex scoring situations can be as Language-Model-as-an-Examiner  (Bai et\u00a0al., 2023 ) , which use Likert scale scoring functions as an absolute evaluative measure. The evaluator assigns scores to a given response along predefined dimensions, including accuracy, coherence, factuality, and comprehensiveness. Each of these dimensions is scored on a scale of 1 to 3, ranging from worst to best. The evaluator is also asked to provide an overall score ranging from 1 to 5, based on the scores assigned to the previous 4 dimensions. This score serves as an indicator of the overall quality of the answer. \n  \n   Figure 3 . The illustrations of method generating scores in ICL      Evaluation Prompt Templates from Gao et\u00a0al. ( 2023b )    Likert Scale Scoring: \n Evaluate the quality of summaries written for a news article. Rate each summary on four dimensions: {Dimension_1} , {Dimension_2} , {Dimension_3} , and {Dimension_4} . You should rate on a scale from 1 (worst) to 5 (best).  Article: {Article}  Summary: {Summary}  Pairwise Comparison: \n Given a new article, which summary is better? Answer \u201dSummary 0\u201d or \u201dSummary 1\u201d. You do not need to explain the reason.  Article: {Article}  Summary 0: {Summary_0}  Summary 1: {Summary_1}   \n     2.1.2. Solving Yes/No questions  \n   A Yes/No question requires a judgment on a given statement, focusing solely on its accuracy. This type of question is simple and direct, providing only two fixed responses\u2014yes or no, true or false\u2014without any additional comparisons or choices. \n  \n   This type of evaluation is often utilized in intermediate processes, creating the conditions for a feedback loop. For example, it promotes a self-optimization cycle, as seen in Reflexion  (Shinn et\u00a0al., 2023 ) , which generates verbal self-reflections to provide valuable feedback for future attempts. In scenarios with sparse reward signals, such as a binary success status (success/fail), the self-reflection model uses the current trajectory and persistent memory to generate nuanced and specific feedback.\nSimilarly, in self-improvement contexts (Tian et\u00a0al., 2024 ) , Yes/No questions can be employed to evaluate custom phrases, such as \"Modification needed.\" and \"No modification needed.\" , facilitating entry into the next cycle.\nMoreover, these evaluations are common for testing knowledge accuracy and assessing whether statements align with established facts (Sun et\u00a0al., 2023c ) , like \u201dGiven a question and the associated retrieved knowledge graph triples (entity, relation, entity), you are asked to answer whether it\u2019s sufficient for you to answer the question with these triples and your knowledge (Yes or No).\u201d A detailed and specific example can be seen in the Figure 4 . \n  \n     Evaluation Prompt Templates for Yes/No and Multiple-Choice Tasks    Yes/No Evaluation: \n Is the sentence supported by the article? Answer \u201dYes\u201d or \u201dNo\u201d.  Article: {Article}  Sentence: {Sentence}  Multiple-Choice Evaluation: \n You are given a summary and some semantic content units. For each semantic unit, choose those can be inferred from the summary, return their number.  Summary: {Summary}  Semantic content units:  1. {SCU_1}  2. {SCU_2}  \u2026\u2026  n. {SCU_n}   \n     2.1.3. Conducting pairwise comparisons  \n   Figure 4 . The illustrations of method Solving Yes/No questions and Conducting pairwise comparisons in ICL    Pairwise comparison refers to comparing two options and selecting which one is superior or more aligned with a specific standard, showed in Figure 4 . It involves making a decision between two options rather than judgement between \u2019yes\u2019 or \u2019no\u2019. The comparison can be subjective or based on objective criteria. This evaluation is a relative evaluation. Pairwise comparison is often used for ranking multiple options or prioritizing them, where several comparisons are made between pairs to identify the better choice or establish a hierarchy. \n  \n   Pairwise comparison is a well-established method that has significantly impacted a variety of fields (Qin et\u00a0al., 2024 ) . As noted by (Liu et\u00a0al., 2024b ) , LLM and human evaluations are more aligned in the context of pairwise comparisons compared to score-based assessments. Numerous studies have demonstrated that pairwise comparative assessments outperform other judging methods in terms of positional consistency (Zheng et\u00a0al., 2023b ; Liusie et\u00a0al., 2024 ) . Furthermore, pairwise comparisons can be extended to more complex relation-based assessment frameworks, such as list-wise comparisons, using advanced ranking algorithms (Qin et\u00a0al., 2024 ; Liu et\u00a0al., 2024b ) , data filtering (Yuan et\u00a0al., 2023b ) . In pairwise comparative assessments, LLM-as-a-Judge is prompted to select the response that better answers the question at hand. To accommodate the possibility of a tie, several option modes are introduced. The Two-Option mode requires judges to choose the better response from two given options. The Three-Option mode introduces an additional choice, allowing judges to indicate a tie if neither response is preferable, as shown in Figure 4 . Evaluations typically involve determining the outcomes of win, tie, or loss for responses (Wang et\u00a0al., 2023d ) through pairwise comparisons, with win rounds counted for each response. The Four-Option mode further expands the choices, allowing judges to classify responses as either a \u201dboth good tie\u201d or a \u201dboth bad tie.\u201d \n  \n     2.1.4. Making multiple-choice selections  \n   Multiple-choice selections involve providing several options, not giving relative choices in pairwise comparison, nor making a yes/no judgment.\nThe evaluator must choose the most appropriate or correct one. This method allows for a broader range of responses compared to true/false questions and can assess deeper understanding or preferences. However, this kind of prompt design is more rare than the first three. \n  \n     Reliability Concerns of In-Context Learning    When leveraging in-context learning, certain issues can surface, potentially impacting the reliability of evaluations. These include the variability of LLM outputs due to minor prompt changes, which can lead to unstable results. Furthermore, score-based assessments often exhibit inconsistent inter-rater reliability, influenced by the inherent randomness of LLM generation and its sensitivity to phrasing. Similarly, evaluation formats like Yes/No or multiple-choice questions are prone to ambiguity in response interpretation. Lastly, LLM-as-a-Judge evaluations may inadvertently reflect biases, such as favoring responses based on their position or length.   \n      2.2. Model Selection  \n    2.2.1. General LLM  \n   To automate evaluation by LLM-as-a-Judge, one effective approach is to employ advanced language models such as GPT-4 (OpenAI, 2023 ) instead of human evaluators (Zheng et\u00a0al., 2023b ) . For instance, Li et\u00a0al. ( 2023c ) created a test set with 805 questions and assessed the performance by comparing it to text-davinci-003 using GPT-4. Additionally, Zheng et\u00a0al. ( 2023b ) designed 80 multi-round test questions across eight common areas and used GPT-4 to automatically score the model\u2019s responses. The accuracy of the GPT-4-based evaluator has been demonstrated to be high compared to professional human evaluators, showing superior consistency and stability in evaluations. At the same time, if the general LLM used has limitations in instruction-following or reasoning abilities, the effectiveness of the LLM-as-a-Judge method may be significantly affected. \n  \n     2.2.2. Fine-tuned LLM  \n   However, relying on external API for evaluation may introduce consideration about privacy leakage, and the opacity of API models also challenges the evaluation reproducibility. Therefore, subsequent studies recommend refining language models tailored for evaluations by emphasizing the use of pairwise comparisons or grading.\nFor instance, PandaLM (Wang et\u00a0al., 2023d ) constructs data based on Alpaca instructions and GPT-3.5 annotation, and then fine-tunes LLaMA-7B (Touvron et\u00a0al., 2023a ) as an evaluator model. JudgeLM (Zhu et\u00a0al., 2023 ) constructs data from diversified instruction sets and GPT-4 annotations, and fine-tunes Vicuna (Touvron et\u00a0al., 2023b ) as a scalable evaluator model. Auto-J (Li et\u00a0al., 2023b ) constructs evaluation data upon multiple scenarios to train a generative evaluator model, which can provide both evaluation and critical opinion. Prometheus (Kim et\u00a0al., 2023 ) defines thousands of evaluation criteria and constructs a feedback dataset based on GPT-4, and fine-tunes a fine-grained evaluator model.\nThe typical process for fine-tuning a judge model involves three main steps. Step 1: Data Collection. The training data generally consists of three components: instructions, the objects to be evaluated, and evaluations. Instructions are typically sourced from instruction datasets, while evaluations can come from either GPT-4 or human annotations. Step 2-Prompt Design. The structure of the prompt template can vary based on the evaluation scheme, which already detailed in \u00a7 2.1 . Step 3: Model Fine-Tuning. Using the designed prompts and collected data, the fine-tuning process for the evaluator model typically adheres to the instruction fine-tuning paradigm (Ouyang et\u00a0al., 2022 ) . The model receives an instruction along with one or more responses to generate output that includes evaluation results and possibly explanations. \n  \n   After fine-tuning, the evaluator model can be employed to evaluate the target object. While these fine-tuned models often demonstrate superior performance on self-designed test sets, they are identified several limitations in their evaluation capabilities,which detailed in Section 4.2 . The current prompt and fine-tuning dataset designs often result in evaluation LLMs with poor generalization, making them difficult to compare with strong LLMs like GPT-4. \n  \n     Reliability Concerns of Model Selection    The choice of model significantly impacts the dependability of LLM-as-a-Judge systems. Concerns arise from the black-box nature and version dependency of general-purpose LLMs, which can hinder the reproducibility of evaluation outputs. Fine-tuned evaluators, while specialized, often exhibit overfitting and limited generalization beyond their training data. Moreover, these models can inherit subtle biases from their training datasets, necessitating careful meta-evaluation to ensure fairness. Finally, reliance on smaller open-source models, while cost-effective, may introduce inconsistencies and misalignment with human judgments.   \n      2.3. Post-processing  \n   Post-processing refines the probability distributions generated by LLM-as-a-Judge to ensure accurate evaluations. The evaluation format should align with our In-Context Learning design and may involve procedures to enhance the reliability of extracted evaluations, which should be applied consistently. We focus on three main post-processing methods: extracting specific tokens, normalizing the output logits, and selecting sentences with high returns.\nHowever, it is important to note that each method has significant limitations when evaluating objective questions. For example, in text response evaluation (Yu et\u00a0al., 2024b ) , failing to accurately extract the key answer token from the LLM\u2019s response can result in incorrect evaluation outcomes. These challenges in post-processing are tightly linked to the prompt design used in earlier ICL stages and the selected model\u2019s ability to follow instructions reliably. \n  \n    2.3.1. Extracting specific tokens  \n   As showed in In-context Learning\u00a0(Section 2.1 ), when the evaluation target take the form of a score, selecting specific options, or responding with Yes/No, applying rule-match to extract the corresponding token from the response generated during probability distribution iteration is common used. It is worth noting that Yes/No is a broad definition, including custom statements involving judgment.\nConsidering a Yes/No question for evaluation in custom phrases (Tian et\u00a0al., 2024 ) : \"Modification needed.\" and \"No modification needed.\" or a yes-no question \"Does the above answer need to be further modified?\" .\nWhen the input sample is put through the template, it might have outputs such as \u201dModification needed.\u201d, \u201dConclusion: Modification needed.\u201d or \u201dYes\u201d. This variance in response formats is difficult to parse consistently. The corresponding post-processing with the response is necessary. Using rules to extract specific tokens for our designed prompts and input content, as well as the backbone model used for the evaluator, all have higher requirements as we discussed in Section 2.2 .\nIn contextual learning, if there is no clear indication of the output format for response, there may be various expressions of evaluation, which can be seen in Figure 2 . For example, \u201dResponse 1 is better\u201d and \u201dThe better one is response 1\u201d, which convey the same choice but differ in format leading to the difficulty of rule recognition. Simple solutions often involve providing clear instructions, such as \u201dThe last sentence should be started with \u2019The better response is\u2019\u201d, or using a few-shot strategy.\nAlso, the general model with insufficient instruction following capability may not be able to generate the evaluation format and content of the target according to the instruction, resulting in the post-processing extracted according to the rules not as smooth as expected. \n  \n   Constrained decoding is a technique that enforces structured output from Large Language Models (LLMs) by restricting token generation according to predefined schemas, typically in formats like JSON. This approach uses a finite state machine (FSM) to compute valid next tokens at each decoding step, effectively masking the model\u2019s output probability distribution to ensure conformity with the desired schema. While this method guarantees syntactically valid outputs, it presents several challenges: it can distort the model\u2019s learned distribution and potentially degrade output quality, requires significant engineering implementation effort, and introduces computational overhead during inference. \n  \n   Recent work has proposed various solutions to address these challenges. (Beurer-Kellner et\u00a0al., 2024 ) introduces DOMINO, a decoding algorithm that preserves natural tokenization while enforcing constraints. Their system minimizes overhead through precomputation and speculative decoding, sometimes achieving faster performance than unconstrained decoding. (Dong et\u00a0al., 2024b ) develops XGrammar, which accelerates grammar-constrained generation by separating tokens into those that can be pre-checked and those requiring runtime verification. By co-designing the grammar engine with LLM inference, they achieve up to 100x speedup over existing approaches. (Zheng et\u00a0al., 2024b ) presents SGLang, combining a domain-specific language with an optimized runtime. Their system features efficient KV cache reuse and compressed finite state machines for faster decoding, demonstrating that thoughtful co-design of programming model and runtime can minimize constrained decoding overhead. \n  \n     2.3.2. Normalizing the output logits  \n   LLM-as-a-Judge in the intermediate steps with Yes/No setting often normalizes the output logits to obtain the evaluation in the form of a continuous decimal between 0 and 1. This is also very common in agent methods and prompt-based optimization methods (Hao et\u00a0al., 2023 ; Zhuang et\u00a0al., 2023 ; Wang et\u00a0al., 2024e ) . For example, the self-consistency and self-reflection scores (Wang et\u00a0al., 2024e ) within one forward pass of \u2133 Evaluator \\mathcal{M}_{\\text{Evaluator}} , are effectively obtained by constructing a prompt [ ( x \u2295 \ud835\udc9e ) , \"Yes\" ] [\\left(x\\oplus\\mathcal{C}\\right),\\texttt{\"Yes\"}] and acquire the probability of each token conditioned on the previous tokens P \u200b ( t i | t < i ) P(t_{i}|t_{<i}) . The auto-regressive feature is leveraged, thus aggregate the probability of the relevant tokens to compute the self-consistent score \u03c1 Self-consistency \\rho_{\\text{Self-consistency}} and self-reflection score \u03c1 Self-reflection \\rho_{\\text{Self-reflection}} . The final score is produced by \u03c1 j = \u03c1 SC , j \u22c5 \u03c1 SR , j \\rho_{j}=\\rho_{\\text{SC},j}\\cdot\\rho_{\\text{SR},j} . \n     ( x \u2295 \ud835\udc9e ) \u23de \u03c1 SC \u200b \"Yes\" \u23de \u03c1 SR \u2192 \u21d2 { \u03c1 SC = \u220f t i \u2208 \u03b1 P \u200b ( t i | t < i ) \u22c5 \u220f t i \u2208 \u03b2 P \u200b ( t i | t < i ) \u03c1 SR = \u220f t i \u2208 \"Yes\" P \u200b ( t i | t < i ) \\displaystyle\\underrightarrow{\\overbrace{\\left(x\\oplus\\mathcal{C}\\right)}^{\\rho_{\\text{SC}}}\\overbrace{\\texttt{\"Yes\"}}^{\\rho_{\\text{SR}}}}\\ \\Rightarrow\\begin{cases}\\rho_{\\text{SC}}=\\prod_{t_{i}\\in\\alpha}P(t_{i}|t_{<i})\\cdot\\prod_{t_{i}\\in\\beta}P(t_{i}|t_{<i})\\\\\n\\rho_{\\text{SR}}=\\prod_{t_{i}\\in\\texttt{\"Yes\"}}P(t_{i}|t_{<i})\\end{cases}     In addition, Self-evaluation (Hao et\u00a0al., 2023 ) is also common using this method for LLM-as-a-Judge. It can be helpful to let the LLM evaluate itself by asking, \u201dIs this reasoning step correct?\u201d and then reward it based on the probability of the next word being \u201dYes.\u201d \n  \n     2.3.3. Selecting sentences  \n   In addition to selecting specific tokens and normalizing the output logits, the content extracted by LLM-as-a-Judge may also be a sentence or paragraph. As showed in Figure 2 , agent for reasoning task (Hao et\u00a0al., 2023 ) , builds a\nreasoning tree by iteratively considering the most promising reasoning steps (actions, sub-questions) by LLM-as-a-Judge. \n  \n     Reliability Concerns of Post-processing Method    The post-processing steps applied to LLM outputs introduce their own set of reliability challenges. Rule-based token extraction methods are inherently brittle and susceptible to minor variations in responses, potentially leading to silent errors. While logit-based normalization offers probabilistic scoring, its effectiveness is highly dependent on precise prompt design and consistent model tokenization, where inconsistencies can introduce noise. Furthermore, methods focusing on sentence-level evaluation or response selection risk propagating biases if the scoring LLM is overly sensitive to stylistic rather than substantive cues. Ultimately, all post-processing strategies remain vulnerable to adversarial manipulations designed to inflate evaluation scores without genuine content improvement.   \n      2.4. Evaluation Pipeline  \n   After completing the three processes, we obtain the final evaluation \u2130 \\mathcal{E} . From input to output, these steps collectively constitute the LLM-as-a-Judge evaluation pipeline, as illustrated in Figure 2 . This pipeline is commonly applied in four scenarios: LLM-as-a-Judge for models, LLM-as-a-Judge for data, LLM-as-a-Judge for agents, and LLM-as-a-Judge for reasoning or thinking. \n  \n   Figure 5 . Four typical scenarios using LLM-as-a-Judge evaluation pipeline.    Figure 6 . The illustrations of the scenario LLM-as-a-Judge for Models . The example of \u201dwin-tie-lose\u201d is from Li et\u00a0al. ( 2023b )     2.4.1. LLM-as-a-Judge for Models  \n   It is universally known that the best way to evaluate LLMs is human judgment, but collecting human annotations can be costly, time-consuming, and laborious (Ouyang et\u00a0al., 2022 ; Zheng et\u00a0al., 2023b ) . Using strong LLMs (usually closed-source ones, e.g., GPT-4, Claude, ChatGPT) as an automated proxy for assessing LLMs has become a natural choice (Zhou et\u00a0al., 2023a ) , as shown in Figure 2 . With appropriate prompt design, the quality of evaluation and agreement to human judgment can be promising (Dubois et\u00a0al., 2023 ; Zheng et\u00a0al., 2023b ; Zhang et\u00a0al., 2023d ; Wang et\u00a0al., 2024c ) .\nHowever, the cost concern still exists when calling the APIs of these proprietary models, especially when there is a frequent need for model validation on large-scale data. Moreover, closed-source LLM-as-a-Judge leads to low reproducibility due to potential changes in models behind the API.\nSome recent works have started to make attempts for open-source alternatives. SelFee (Ye et\u00a0al., 2023 ) collects generations, feedback, and revised generations from ChatGPT and fine-tunes LLaMA models to build a critique model. Shepherd (Wang et\u00a0al., 2023c ) trains a model that can output critiques for single-response with the data of feedback from online communities and human annotation. PandaLM (Wang et\u00a0al., 2023d ) trains a model to conduct pairwise comparison for LLM Instruction Tuning Optimization, and Zheng et\u00a0al. ( 2023b ) also fine-tune Vicuna (Touvron et\u00a0al., 2023b ) on a 20K pairwise comparison dataset to explore the potential of open-source models as a more cost-friendly proxy. \n  \n     2.4.2. LLM-as-a-Judge for Data  \n   Data annotation generally refers to the labeling or generating of raw data with relevant information, which could be used for improving the efficacy of machine learning models. The process, however, is labor-intensive and costly. The emergence of LLMs presents an unprecedented opportunity to automate the complicated process of data annotation by LLM-as-a-Judge.\nMost of the data need to be evaluated by LLM-as-a-Judge is generated by models, or large-scale crawled data.\nLanguage models first conduct supervised fine-tuning to imitate how to align with human instructions (Wang et\u00a0al., 2023a ; Taori et\u00a0al., 2023 ) .\nAfter that, reinforcement learning techniques have been explored to align language models with human preferences (Ouyang et\u00a0al., 2022 ; Ramamurthy et\u00a0al., 2023 ) .\nThe most successful way is applying a RLHF framework (Ouyang et\u00a0al., 2022 ) via training a reward model on human feedback and using PPO (Schulman et\u00a0al., 2017 ) to obtain the policy model for language generation.\nHowever, in practices, the PPO training paradigm is complex in coding and hyper-parameter tuning while it needs four models that are hard for training.\nThis motivates us to explore simpler and more straightforward methods to align language models with human preferences. This involves how to use LLM-as-a-Judge to evaluate whether different responses are aligned with human preferences.\nFor example, (Yuan et\u00a0al., 2023b ; Dong et\u00a0al., 2023 ) use general LLM\u00a0(ChatGPT) to get better alignment with human preferences. The Aplaca prompts (Taori et\u00a0al., 2023 ) is used as sampling queries to different models generate responses. And these data was evaluated by LLM-as-a-Judge to obtain human preference scores\u00a0(reward score) to train a new language model. Other works would like to use Supervised Fine-Tuning\u00a0(SFT) model itself as evaluator, like generating better-aligned datasets for SFT including hindsight-modified prompts (Zhang et\u00a0al., 2023a ; Liu et\u00a0al., 2023d ) and principle-driven self-alignment (Sun et\u00a0al., 2023b ) . \n  \n   Figure 7 . LLM-as-a-Judge appears in two common forms in the agent. The left diagram is Agent-as-a-Juge, designing a complete agent to serve as an evaluator. The right diagram shows using LLM-as-a-Judge in the process of an Agent.    In addition, the lack of domain-specific model training data is a common phenomenon. In order to obtain annotated high-quality data, it is also very common to use LLM-as-a-Judge for the generation and evaluation of domain data. WizardMath  (Luo et\u00a0al., 2023a ) would use its Instruction Reward Model (IRM) as Evaluator, aiming to judge the quality of the evolved instructions on three aspects: i) Definition, ii) Precision, and iti) Integrity. To produce the ranking list training data of IRM, for each instruction, ChatGPT and Wizard-E are used to generate 2-4 evolved instructions respectively. Then we leverage Wizard-E to rank the quality of those 4-8 instructions. \n  \n   However, solely relying on LLM-as-a-Judge for data annotation poses challenges, particularly as the value of annotated data diminishes with the rapid improvement of model performance. To address this, approaches like Self-Taught Evaluator (Wang et\u00a0al., 2024b ) offer a promising alternative by eliminating the need for human annotations. This method leverages synthetic training data, starting with unlabeled instructions and generating contrasting outputs from models. These outputs are then used to train an LLM-as-a-Judge to produce reasoning traces and final judgments. With each iteration, the evaluator improves by learning from its refined predictions, creating a cycle of continuous self-enhancement. This iterative approach not only keeps annotations relevant but also ensures that evaluators evolve alongside advancing models. \n  \n   Recent research on evaluating multimodal data focuses on addressing vision-language misalignments in Multimodal Large Language Models (MLLMs), which often cause hallucinations\u2014outputs inconsistent with visual or contextual evidence (Li et\u00a0al., 2023a ; Wang et\u00a0al., 2023f ; Cui et\u00a0al., 2023 ) . Techniques like RLHF and Factually Augmented RLHF have been employed to improve model alignment by incorporating structured ground-truth data and image captions, enhancing hallucination detection (Sun et\u00a0al., 2023a ) . Benchmarks such as MLLM-as-a-Judge (Chen et\u00a0al., 2024b ) assess these models using tasks like scoring, pair comparison, and batch ranking, revealing limitations in alignment with human preferences. Persistent issues include biases (e.g., position, verbosity) and hallucinations, with even advanced models like GPT-4V displaying challenges. While pair comparison tasks align better with human judgment, scoring and batch ranking require significant improvements for reliable deployment. These findings emphasize the need for innovative frameworks and datasets to refine MLLM evaluation and alignment. \n  \n     2.4.3. LLM-as-a-Judge for Agents  \n   There are two ways to apply LLM-as-a-Judge for an agent. One is to evaluate the entire process of the intelligent agent (Zhuge et\u00a0al., 2024 ) , and the other is to evaluate it at a specific stage in the agent framework process (Hao et\u00a0al., 2023 ; Shinn et\u00a0al., 2023 ) . Both approaches are briefly illustrated in Figure 7 .\nUsing LLM as the brain of agent, an agentic system (Zhuge et\u00a0al., 2024 ) could evaluate like a human, it would reduce the need for human involvement and eliminate the trade-off between thoroughness and effort. In addition, the agent (Shinn et\u00a0al., 2023 ) can interact with the environment through language and receive feedback on actions through LLM to make decisions for the next action. \n  \n     2.4.4. LLM-as-a-Judge for Reasoning/Thinking  \n   Reasoning (Huang and Chang, 2023 ) , defined as the cognitive process of applying logic, arguments, and evidence to draw conclusions, is central to intellectual tasks such as decision-making, problem-solving, and critical analysis.\nWhile reasoning is inherently more demanding and multifaceted than judging, it often depends on judgments to ensure logical coherence, refine intermediate steps, and achieve clarity in its outcomes. LLM-as-a-Judge, in this sense, becomes an integral tool for enhancing the reasoning capability of LLM. \n  \n   The role of LLM-as-a-Judge in enhancing reasoning or thinking can be understood through two frameworks: scaling training time (Gao et\u00a0al., 2023c ; Trung et\u00a0al., 2024 ) and scaling test time (Snell et\u00a0al., 2024 ) . In the training phase, LLM-as-a-Judge frequently operates within reinforcement learning paradigms, where it functions as a reward model or evaluator for data or processes. This enables the creation of high-quality reasoning datasets through mechanisms such as step-by-step verification (Lightman et\u00a0al., 2023 ) , Direct Preference Optimization(DPO) (Rafailov et\u00a0al., 2024 ) , and self-refinement (Yuan et\u00a0al., 2024 ) . Recently, several LLMs trained with reinforcement learning to exhibit advanced reasoning and thinking abilities have gained attention, such as o1 2 2 2 https://openai.com/index/learning-to-reason-with-llms/ , DeepSeek-R1 3 3 3 https://api-docs.deepseek.com/news/news1120 ,gemini-thinking 4 4 4 https://ai.google.dev/gemini-api/docs/thinking-mode , and QVQ 5 5 5 https://huggingface.co/Qwen/QVQ-72B-Preview .\nIn the test-time framework, LLM-as-a-Judge is crucial for evaluating and selecting the best reasoning paths. For example, in \u201dBest-of-N\u201d generation scenarios, where multiple reasoning outputs are produced, the judge determines the most accurate and coherent response. This dual role in both training and test phases demonstrates the indispensable nature of LLM-as-a-Judge in enhancing reasoning systems. \n  \n     Reliability Concerns of Evaluation Pipeline    Within the broader evaluation pipeline, several factors can compromise the integrity of LLM-based assessments. When evaluating models, inherent biases such as position bias and self-enhancement can significantly impact fairness, requiring careful mitigation strategies. In data evaluation contexts, the extensive use of LLM-as-a-Judge for pseudo-labeling risks amplifying existing model biases, potentially leading to the generation of unverified and flawed training data. For agentic frameworks, the sequential application of LLM-as-a-Judge can lead to the accumulation of errors, where early misjudgments cascade into significant inaccuracies in final decisions. Moreover, evaluating complex reasoning tasks poses a particular risk; without robust verification of logical consistency, LLMs may produce seemingly confident but fundamentally flawed evaluations, undermining trust in sophisticated analytical processes.   \n      2.5. Quick Practice \n   To effectively apply LLM-as-a-Judge design, it is recommended to find more effective configurations in the testing cycle for various scenarios. Crucially, reliable testing should form the bedrock of this quick practice, necessitating iterative refinement, continuous feedback loops, and the establishment of clear reliability metrics. The overarching aim is to continuously optimize evaluation stability and consistency through dedicated, reliability-focused testing cycles. The success of using LLM-as-a-Judge also heavily depends on the implementation details, including the task complexity, the prompt design, the model selected, and the post-processing method. \n  \n   Figure 8 . Flowchart of Quick Practice    As shown in Figure 8 , The process of quick practice for LLM-as-a-Judge involves four main stages.\nFirst is the thinking phase, in which users define the evaluation objectives by determining what needs to be evaluated, understanding typical human evaluation approaches, and identifying some reliable evaluation examples. This initial conceptualization is vital for preempting potential ambiguities and biases that could compromise the fairness and accuracy of subsequent LLM judgments. \n  \n   Next is prompt design, detailed in Section 2.1 , where both wording and formats matter. The most efficient and generally effective approach involves specifying scoring dimensions, emphasizing relative comparisons for improved assessments, and creating effective examples to guide the LLM. Careful prompt engineering is essential to mitigate issues like output variability and inter-rater reliability, ensuring the LLM consistently interprets and responds to evaluation criteria as intended. \n  \n   The third stage, model selection (Section 2.2 ), focuses on choosing a large-scale model with strong reasoning and instruction-following abilities to ensure reliable evaluations. The inherent biases and generalization limitations of different models, as well as their black-box nature, underscore the importance of selecting a robust and well-understood backbone to minimize evaluation inconsistencies and unverified judgments. \n  \n   Finally, standardizing the evaluation process ensures that the outputs are structured\u00a0(Section 2.3 ). This can be achieved by using specific formats like \\boxed{XX}, numerical scores, or binary responses (e.g., \u201dYes\u201d or \u201dNo\u201d). Such standardization is crucial to counteract the fragility of token extraction methods and the potential for stylistic biases to propagate, thereby enhancing the interpretability and validity of the final evaluation results. The entire process includes iterative testing with cases and refinement through retesting, thereby enhancing reliability. During development, it is essential to compare models or prompts and verify ongoing improvements. \n  \n      3. Improvement Strategy \n   When directly utilizing LLMs to conduct evaluation tasks\u2014such as scoring, selection, pairwise comparison, or ranking\u2014their inherent biases of LLMs like length bias, position bias, and co\n[truncated]",
  "title": "A Survey on LLM-as-a-Judge",
  "content_type": "text/html; charset=utf-8"
}