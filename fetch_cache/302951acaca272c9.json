{
  "url": "https://levelup.gitconnected.com/mimicking-claude-code-cli-using-langgraph-anthropic-and-fastapi-467a2ec3ff0e",
  "type": "webpage",
  "content": "Mimicking Claude Code CLI Using LangGraph, Anthropic, and FastAPI | by Plaban Nayak | Level Up Coding Sitemap \n Open in app Sign up \n Sign in \n \n \n \n Medium Logo \n \n \n \n \n Write \n \n Search \n \n Sign up \n Sign in \n \n \n \n \n \n \n \n \n \n \n \n \n \n Level Up Coding \n \n \n \u00b7 Follow publication \n \n \n \n \n \n \n \n \n \n Coding tutorials and news. The developer homepage gitconnected.com && skilled.dev && levelup.dev \n \n Follow publication \n \n \n \n \n \n \n \n \n \n Mimicking Claude Code CLI Using LangGraph, Anthropic, and FastAPI \n \n \n \n \n \n \n \n Plaban Nayak \n \n \n \n \n \n \n \n 25 min read \u00b7 \n Nov 9, 2025 \n \n \n \n \n \n \n \n -- \n \n \n \n 1 \n \n \n \n \n \n \n \n \n \n Listen \n \n \n \n \n \n \n \n \n \n \n Share \n \n \n \n \n \n \n \n \n \n \n \n \n \n Press enter or click to view image in full size \n \n Minimalistic Mimic -Claude Code \n \n \n Introduction \n Imagine having an AI assistant that doesn\u2019t just suggest code snippets, but actually generates complete, production-ready applications with proper file structures, comprehensive documentation, and best practices baked in. Welcome to the world of AI-powered code generation \u0432\u0496\u201d where natural language meets executable code. \n In this article, we\u2019ll dive deep into building a sophisticated code generation system that mimics the powerful Claude Code CLI. We\u2019ll leverage LangGraph for workflow orchestration, Anthropic\u2019s Claude Sonnet 4.5 for intelligent code generation, and FastAPI for a modern web interface. By the end, you\u2019ll understand how to build an agentic system that can generate entire codebases from simple prompts. \n What makes this project special? \n Agentic Architecture : Uses LangGraph\u2019s StateGraph for intelligent decision-making \n Tool-Augmented LLM : Claude doesn\u2019t just chat \u0432\u0496\u201d it executes tools to create files, analyze code, and build projects \n Dual Interface : Both CLI and web-based interactions with WebSocket support \n Stateful Conversations : SQLite checkpointing enables context persistence across sessions \n Production-Ready : Error handling, validation, and graceful degradation \n Architecture Overview \n The Big Picture \n Our system follows a multi-agent workflow architecture where an AI agent orchestrates between thinking (model responses) and acting (tool execution). Here\u2019s how the components fit together: \n \n \n Press enter or click to view image in full size \n \n \n \n \n System Architecture Overview \n Press enter or click to view image in full size \n \n Core Components: \n User Interface Layer \n Rich Terminal UI / Web UI for user interaction \n 2. Entry Point \n main.py / app.py - Application initialization \n 3. Core Agent System \n CodeGeneratorAgent - Main agent controller \n StateGraph Workflow - Manages execution flow \n Claude Sonnet 4.5 - Primary LLM for code generation \n 4. Workflow Nodes \n model_response - Handles LLM responses \n tool_use - Manages tool execution \n check_tool_use - Determines if tools are needed \n 5. Tools Layer \n Code Generation Tools \n File Operation Tools \n 6. Persistence \n SQLite Checkpointer with database storage \n 7. Output \n File system writing to ./generated_code/ directory \n StateGraph Workflow Flow \n \n User input \u2192 Main application \u2192 CodeGeneratorAgent \n StateGraph orchestrates between nodes: \n model_response gets LLM completion \n check_tool_use evaluates if tools are required \n If tools needed \u2192 tool_use executes appropriate tools \n If no tools needed \u2192 Process ends \n 3. Tools interact with file system to generate code \n 4. Checkpointing maintains state persistence \n Key Features: \n State Management: SQLite-based checkpointing for resilience \n Tool Integration: Both code generation and file operations \n LLM Integration: Claude Sonnet 4.5 as the brain \n Modular Design: Clear separation of concerns \n Message Flow Visualization \n Understanding how messages flow through the system is crucial. Here\u2019s the complete lifecycle: \n 1. User provides natural language description \n 2. System message is added (first message only) \n 3. LLM processes input and decides on tool usage \n 4. If tools needed: execute tools \u2192 return to model response \n 5. If no tools: display final response \u2192 return to user input \n 6. State is checkpointed after each step \n Press enter or click to view image in full size \n \n Key Insight : Each message type serves a specific purpose: \n HumanMessage : User input \n SystemMessage : Instructions for the LLM \n AIMessage : LLM responses (with optional tool_calls) \n ToolMessage : Results from tool execution \n Component Interaction Matrix \n Press enter or click to view image in full size \n \n Performance Optimization Strategies: \n \u2014 \u26a1 Async I/O : All file operations and LLM calls are async \n \u2014 \ud83d\udcbe Efficient Checkpointing : Only save state after node completion \n \u2014 \ud83d\udd04 Streaming : WebSocket implementation streams tokens in real-time \n \u2014 \ud83d\udce6 Tool Batching : Multiple tool calls executed in parallel when possible \n Technology Stack \n \ud83e\udde0 Core AI/ML \n LangGraph (Workflow Orchestration) \u2014 Why? LangGraph provides a declarative way to build stateful, multi-step AI workflows. Unlike simple LLM chains, it supports conditional routing, cycles, and complex decision trees. \u2014 Role : Orchestrates the agent\u2019s think-act cycle with conditional edges and state persistence. \n Anthropic Claude Sonnet 4.5 (Language Model) \u2014 Why? Claude excels at following complex instructions, structured output, and code generation. The 4.5 version offers enhanced reasoning capabilities. \u2014 Configuration : Temperature 0.7 for creative yet consistent code, 8192 token limit for large code files. \n LangChain (LLM Framework) \u2014 Why? Provides standardized interfaces for LLM interaction, message handling, and tool integration. \u2014 Components Used : ChatAnthropic, BaseMessage types, tool decorators. \n \ud83c\udf10 Web Framework \n FastAPI (Async Web Server) \u2014 Why? FastAPI\u2019s async support is perfect for streaming LLM responses. Built-in validation with Pydantic, automatic OpenAPI docs, and WebSocket support. \u2014 Features Used : WebSockets for real-time streaming, CORS middleware, background tasks, dependency injection. \n Uvicorn (ASGI Server) \u2014 Why? Production-grade async server with hot reload for development. \n \ud83d\udcbe State Management \n AsyncSqliteSaver (LangGraph Checkpointing) \u2014 Why? Enables conversation persistence, debugging, and resume-from-checkpoint functionality. \u2014 Storage : Local SQLite database with async operations. \n \ud83c\udfa8 User Interface \n Rich (CLI Rendering) \u2014 Why? Beautiful terminal output with markdown rendering, panels, trees, and spinners. \u2014 Components : Console, Panel, Markdown, Tree, Prompt. \n WebSockets (Real-Time Communication) \u2014 Why? Enables streaming LLM responses to the browser in real-time, creating a ChatGPT-like experience. \n \ud83d\udee0\ufe0f Development Tools \n Python-dotenv : Environment variable management \n Pydantic : Data validation and settings management \n asyncio : Async/await support for concurrent operations \n Required Dependencies \n Here\u2019s the complete requirements.txt for the project: \n # Core AI/ML Dependencies \n langchain==0.2.0 \n langchain-core==0.2.0 \n langchain-anthropic==0.1.15 \n langgraph==0.1.0 \n anthropic==0.25.0 \n  \n # Web Framework \n fastapi==0.111.0 \n uvicorn[standard]==0.30.0 \n websockets==12.0 \n python-multipart==0.0.9 \n  \n # State Management \n aiosqlite==0.20.0 \n  \n # CLI/UI \n rich==13.7.1 \n prompt-toolkit==3.0.43 \n  \n # Utilities \n python-dotenv==1.0.1 \n pydantic==2.7.0 \n pydantic-settings==2.2.1 \n  \n # Development (Optional) \n pytest==8.2.0 \n pytest-asyncio==0.23.7 \n black==24.4.2 Installation: \n pip install -r requirements.txt Environment Setup: Create a .env file: \n ANTHROPIC_API_KEY=your_api_key_here \n OUTPUT_DIR=./generated_code \n PORT=8000 Code Workflow: A Deep Dive \n 1. Entry Point: main.py (CLI Interface) \n The CLI version provides a terminal-based interface with rich formatting: \n async def main(): \n # Load environment and validate API key \n load_dotenv() \n if not os.getenv(\"ANTHROPIC_API_KEY\"): \n console.print(\"[bold red]\u274c Error: ANTHROPIC_API_KEY not found![/bold red]\") \n sys.exit(1) \n \n # Initialize output directory \n output_dir = os.getenv(\"OUTPUT_DIR\", \"./generated_code\") \n os.makedirs(output_dir, exist_ok=True) \n \n # Create and initialize agent \n agent = CodeGeneratorAgent() \n await agent.initialize()  # Async initialization for SQLite \n \n # Run interactive loop \n await agent.run() Key Features: \n \u2705 API key validation upfront \n \ud83d\udcc1 Auto-creation of output directories \n \ud83d\udd04 Async initialization pattern \n \ud83c\udfaf Clean error handling with graceful shutdown \n 2. Web Interface: app.py (FastAPI + WebSockets) \n The web version supports multiple concurrent users with isolated sessions \n 2. Web Interface: app.py (FastAPI + WebSockets) \n \n The web version supports multiple concurrent users with isolated sessions: \n \n class AgentManager: \n \"\"\"Manages multiple agent instances for concurrent users\"\"\" \n def __init__(self): \n self.agents: Dict[str, CodeGeneratorAgent] = {} \n self.agent_configs: Dict[str, dict] = {} \n \n async def get_or_create_agent(self, session_id: str) -> CodeGeneratorAgent: \n if session_id not in self.agents: \n agent = CodeGeneratorAgent() \n agent.console = SilentConsole()  # Suppress terminal output \n await agent.initialize() \n self.agents[session_id] = agent \n self.agent_configs[session_id] = { \n \"configurable\": {\"thread_id\": f\"web_session_{session_id}\"} \n } \n return self.agents[session_id] Architectural Highlights: \n Session Isolation : Each user gets their own agent instance with isolated state \n Graceful Degradation : Silent console for web to avoid terminal output \n Thread-Safe Operations : Async context managers for resource cleanup \n Lifespan Management : Proper startup/shutdown hooks with @asynccontextmanager \n WebSocket Streaming: \n @app.websocket(\"/ws/{session_id}\") \n async def websocket_endpoint(websocket: WebSocket, session_id: str): \n await manager.connect(websocket) \n \n # Stream workflow execution \n async for event in agent.agent.astream({\"messages\": [human_message]}, config): \n for node_name, node_output in event.items(): \n if node_name == \"model_response\": \n # Stream AI responses in real-time \n await manager.send_personal_message({ \n \"type\": \"response\", \n \"content\": str(msg.content), \n \"node\": node_name \n }, websocket) @app.websocket(\"/ws/{session_id}\") \n async def websocket_endpoint(websocket: WebSocket, session_id: str): \n await manager.connect(websocket) \n \n # Stream workflow execution \n async for event in agent.agent.astream({\"messages\": [human_message]}, config): \n for node_name, node_output in event.items(): \n if node_name == \"model_response\": \n # Stream AI responses in real-time \n await manager.send_personal_message({ \n \"type\": \"response\", \n \"content\": str(msg.content), \n \"node\": node_name \n }, websocket) This creates a real-time streaming experience where users see the AI \"thinking\" and \"acting\" as it happens. \n 3. The Brain: agent.py (LangGraph Workflow) \n This is where the magic happens. The agent orchestrates a stateful workflow with three key nodes: \n State Definition \n class AgentState(BaseModel): \n \"\"\"State management for the code generator workflow\"\"\" \n messages: Annotated[Sequence[BaseMessage], add_messages] Simple but powerful: the state is just a list of messages. LangGraph\u2019s add_messages automatically handles message accumulation. \n Workflow Graph Construction \n def _setup_workflow(self): \n # Register nodes \n self.workflow.add_node(\"model_response\", self.model_response) \n self.workflow.add_node(\"tool_use\", self.tool_use) \n \n # Define edges \n self.workflow.set_entry_point(\"model_response\") \n self.workflow.add_edge(\"tool_use\", \"model_response\") \n \n # Conditional routing \n self.workflow.add_conditional_edges( \n \"model_response\", \n self.check_tool_use, \n { \n \"tool_use\": \"tool_use\", \n END: END, \n }, \n ) Workflow Visualization \n Press enter or click to view image in full size \n \n Node 1: model_response (AI Thinking) \n def model_response(self, state: AgentState) -> dict: \n messages = state.messages \n \n # Add system message on first interaction \n if len(messages) == 1: \n system_message = SystemMessage(content=\"\"\"You are an expert AI code generator... \n \n CRITICAL RULES: \n 1. When generating code, ALWAYS write it to files using write_file \n 2. MUST provide BOTH file_path AND content parameters \n 3. Generate complete, runnable code - not snippets \n ... \n \"\"\") \n messages = [system_message] + list(messages) \n \n # Invoke LLM with tools bound \n response = self.llm_with_tools.invoke(messages) \n \n return {\"messages\": [response]} Key Innovation: System Prompt Engineering \n The system prompt explicitly teaches the LLM: \n \u2705 When to use tools (always for code generation) \n \u2705 Required parameters for each tool \n \u2705 Workflow patterns (generate \u2192 write \u2192 confirm) \n \u2705 Error recovery strategies \n Node 2: tool_use (AI Acting) \n def tool_use(self, state: AgentState) -> dict: \n messages = state.messages \n last_message = messages[-1] \n tool_calls = last_message.tool_calls \n tool_messages = [] \n \n for tool_call in tool_calls: \n tool_name = tool_call[\"name\"] \n tool_args = tool_call[\"args\"].copy() \n \n # \ud83d\udd27 AUTO-FIX MECHANISM: Extract missing content from prior responses \n if tool_name == \"write_file\" and not tool_args.get(\"content\"): \n # Try to extract code blocks from previous AI response \n content = self._extract_code_from_response(last_response_content) \n if content: \n tool_args[\"content\"] = content \n \n # Execute tool \n tool = next((t for t in self.tools if t.name == tool_name), None) \n result = tool.invoke(tool_args) \n \n tool_messages.append(ToolMessage( \n content=str(result), \n tool_call_id=tool_call[\"id\"] \n )) \n \n return {\"messages\": tool_messages} Error Recovery Innovation: \n The system includes an auto-fix mechanism that attempts to extract missing parameters (like file content) from previous messages. If the LLM forgets to include content, the agent tries to recover by parsing code blocks from the conversation history. \n Conditional Edge: check_tool_use \n def check_tool_use(self, state: AgentState) -> Literal[\"tool_use\", END]: \n last_message = state.messages[-1] \n \n if hasattr(last_message, \"tool_calls\") and last_message.tool_calls: \n return \"tool_use\"  # Continue to tool execution \n return END  # No tools \u2192 conversation ends Simple but essential: routes the workflow based on whether the LLM requested tool calls. \n 4. Tools: The AI\u2019s Hands \n Tools are defined using LangChain\u2019s @tool decorator, which automatically generates JSON schemas for the LLM. \n Code Generation Tools (code_tools.py) \n generate_code : Signals code generation intent \n @tool \n def generate_code(description: str, language: str = \"python\", file_path: str = None) -> str: \n \"\"\"Generate code from a natural language description.\"\"\" \n return f\"Code generation requested for: {description}\\nLanguage: {language}\" create_project_structure : Multi-file project creation \n @tool \n def create_project_structure(project_name: str, structure: str) -> str: \n \"\"\"Create a complete project structure with multiple files and directories. \n \n Args: \n project_name: Name of the project \n structure: JSON string describing files and directories \n \"\"\" \n structure_data = json.loads(structure) \n output_dir = os.getenv(\"OUTPUT_DIR\", \"./generated_code\") \n project_path = os.path.join(output_dir, project_name) \n \n # Recursively create files and directories \n for item_name, item_data in structure_data.items(): \n create_item(item_name, item_data) \n \n return f\"\u2713 Created project '{project_name}' with {len(created_files)} items\" analyze_code : Read and analyze existing code \n @tool \n def analyze_code(file_path: str) -> str: \n \"\"\"Analyze existing code and provide suggestions for improvement.\"\"\" \n with open(file_path, 'r', encoding='utf-8') as f: \n content = f.read() \n return f\"Code from {file_path}:\\n\\n{content}\\n\\nPlease analyze this code.\" File Operation Tools (file_tools.py) \n write_file : Core file creation tool \n @tool \n def write_file(file_path: str, content: str) -> str: \n \"\"\"Write content to a file. Creates directories if needed. \n **REQUIRED PARAMETERS:** \n - file_path: Path to write to \n - content: **REQUIRED** - Complete file content as string \n \"\"\" \n # Convert to absolute path \n if not os.path.isabs(file_path): \n file_path = os.path.join(os.getcwd(), file_path) \n # Create parent directories \n os.makedirs(os.path.dirname(file_path), exist_ok=True) \n # Write file \n with open(file_path, 'w', encoding='utf-8') as f: \n f.write(content) \n return f\"\u2713 Successfully wrote {len(content)} characters to {file_path}\" read_file, list_files, create_directory, search_files : Standard file operations with robust error handling. \n 5. Checkpointing: SQLite-Based State Persistence \n async def initialize(self): \n db_path = os.path.join(os.getcwd(), \"code_generator_checkpoints.db\") \n # Initialize async SQLite checkpointer \n self._checkpointer_ctx = AsyncSqliteSaver.from_conn_string(db_path) \n self.checkpointer = await self._checkpointer_ctx.__aenter__() \n # Compile workflow with checkpointer \n self.agent = self.workflow.compile(checkpointer=self.checkpointer) Benefits: \n \ud83d\udcbe Persistence : All conversations saved automatically \n \ud83d\udc1b Debugging : Replay workflows step-by-step \n \ud83d\udd04 Resume : Continue interrupted sessions \n \ud83d\udcca Analytics : Analyze agent behavior over time \n Database Schema (auto-generated): \n CREATE TABLE checkpoints ( \n thread_id TEXT, \n checkpoint_id TEXT, \n parent_checkpoint_id TEXT, \n state BLOB,  -- Serialized AgentState \n metadata JSON, \n PRIMARY KEY (thread_id, checkpoint_id) \n ); State Persistence Visualization \n Here\u2019s how conversation state flows through the persistence layer: \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502  AgentState                                             \u2502 \n \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500-\u2510 \u2502 \n \u2502  \u2502 messages: [                                        \u2502 \u2502 \n \u2502  \u2502   SystemMessage(\"You are...\"),                     \u2502 \u2502 \n \u2502  \u2502   HumanMessage(\"Generate API\"),                    \u2502 \u2502 \n \u2502  \u2502   AIMessage(\"I'll generate...\"),                   \u2502 \u2502 \n \u2502  \u2502   ToolMessage(\"Code generated...\"),                \u2502 \u2502 \n \u2502  \u2502   AIMessage(\"Done!\")                               \u2502 \u2502 \n \u2502  \u2502 ]                                                  \u2502 \u2502 \n \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500-\u2518 \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502 \n \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502  AsyncSqliteSaver                                       \u2502 \n \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500-\u2510 \u2502 \n \u2502  \u2502 Thread ID: \"code_generator_session\"                \u2502 \u2502 \n \u2502  \u2502 Checkpoint ID: \"abc123...\"                         \u2502 \u2502 \n \u2502  \u2502 Save: messages, metadata, timestamps               \u2502 \u2502 \n \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500-\u2518 \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502 \n \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502  SQLite Database                                        \u2502 \n \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500-\u2510 \u2502 \n \u2502  \u2502 code_generator_checkpoints.db                      \u2502 \u2502 \n \u2502  \u2502                                                    \u2502 \u2502 \n \u2502  \u2502 Tables:                                            \u2502 \u2502 \n \u2502  \u2502  \u2022 checkpoints                                     \u2502 \u2502 \n \u2502  \u2502  \u2022 writes                                          \u2502 \u2502 \n \u2502  \u2502                                                    \u2502 \u2502 \n \u2502  \u2502 Enables:                                           \u2502 \u2502 \n \u2502  \u2502  \u2022 Conversation resume                             \u2502 \u2502 \n \u2502  \u2502  \u2022 Debugging & replay                              \u2502 \u2502 \n \u2502  \u2502  \u2022 State inspection                                \u2502 \u2502 \n \u2502  \u2502  \u2022 Time-travel debugging                           \u2502 \u2502 \n \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500-\u2518 \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Advanced Feature: Time-Travel Debugging \n With checkpointing, you can replay any conversation from any checkpoint: \n # Get all checkpoints for a thread \n checkpoints = agent.checkpointer.list(\"code_generator_session\") \n # Resume from a specific checkpoint \n config = { \n \"configurable\": { \n \"thread_id\": \"code_generator_session\", \n \"checkpoint_id\": checkpoints[2].id  # Go back to checkpoint 2 \n } \n } \n # Continue from that point \n agent.invoke({\"messages\": [HumanMessage(\"Try a different approach\")]}, config) Error Handling Architecture \n The system includes robust error handling at every layer: \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502  Tool Execution                                         \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502 \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502                           \u2502 \n \u25bc                           \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502 Success       \u2502         \u2502 Error         \u2502 \n \u2502 Return Result \u2502         \u2502 Catch Exception\u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502                         \u2502 \n \u2502                         \u25bc \n \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502              \u2502 Auto-Fix Attempt \u2502 \n \u2502              \u2502 (Extract params) \u2502 \n \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502                       \u2502 \n \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502              \u2502                 \u2502 \n \u2502              \u25bc                 \u25bc \n \u2502         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502         \u2502 Fixed   \u2502      \u2502 Failed  \u2502 \n \u2502         \u2502 Retry   \u2502      \u2502 Create  \u2502 \n \u2502         \u2502 Tool    \u2502      \u2502 Error   \u2502 \n \u2502         \u2502         \u2502      \u2502 Message \u2502 \n \u2502         \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \n \u2502              \u2502                \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502 \n \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502 ToolMessage         \u2502 \n \u2502 (Success or Error)  \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502 \n \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502 Add to State        \u2502 \n \u2502 Continue Workflow   \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502 \n \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502 LLM Processes       \u2502 \n \u2502 Error feedback helps\u2502 \n \u2502 LLM self-correct    \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Error Recovery Example: \n When the LLM forgets to include file content: \n Detection : Tool validation catches missing content parameter \n Auto-Fix : System extracts code blocks from previous AI response \n Retry : Tool re-invoked with extracted content \n Success : File written successfully \n Learning : LLM sees the pattern and adjusts future calls \n This creates a self-improving feedback loop where the LLM learns from its mistakes within the same conversation. \n Understanding MCP Tools \n MCP (Model Context Protocol) is a standardized way to provide tools to language models. In our implementation: \n Tool Definition Pattern \n @tool \n def write_file(file_path: str, content: str) -> str: \n \"\"\" \n Docstring becomes the tool description shown to the LLM. \n Parameters are extracted from the function signature. \n Type hints generate JSON schema for validation. \n \"\"\" \n # Implementation \n return \"Result shown to LLM\" How LLMs Use Tools \n Discovery : LLM receives tool schemas during initialization \n { \n \"name\": \"write_file\", \n \"description\": \"Write content to a file...\", \n \"parameters\": { \n \"type\": \"object\", \n \"properties\": { \n \"file_path\": {\"type\": \"string\"}, \n \"content\": {\"type\": \"string\"} \n }, \n \"required\": [\"file_path\", \"content\"] \n } \n } 2. Decision : LLM decides when to use tools based on user request \n 3. Invocation : LLM generates structured tool calls \n { \n \"tool_call\": { \n \"id\": \"call_123\", \n \"name\": \"write_file\", \n \"args\": { \n \"file_path\": \"./app.py\", \n \"content\": \"def hello():\\n    print('Hello!')\" \n } \n } \n } 4. Execution : Agent invokes tool and returns result to LLM \n Get Plaban Nayak \u2019s stories in\u00a0your\u00a0inbox \n \n Join Medium for free to get updates from\u00a0this\u00a0writer. \n \n \n \n Subscribe \n \n Subscribe \n \n \n \n \n 5. Iteration : LLM sees result and decides next action \n Tool Execution Flow Diagram \n Here\u2019s a detailed visualization of how tools are discovered, invoked, and executed: \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502  LLM Decides: Need Tools                                \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502 \n \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502  Extract tool_calls from AIMessage                      \u2502 \n \u2502  [                                                      \u2502 \n \u2502    {name: \"generate_code\", args: {...}},                \u2502 \n \u2502    {name: \"write_file\", args: {...}}                    \u2502 \n \u2502  ]                                                      \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502 \n \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502                           \u2502 \n \u25bc                           \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502 Code Tool     \u2502         \u2502 File Tool     \u2502 \n \u2502 generate_code \u2502         \u2502 write_file    \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502                         \u2502 \n \u2502                         \u2502 \n \u25bc                         \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502 Plan Code     \u2502         \u2502 File System   \u2502 \n \u2502 Structure     \u2502         \u2502 Write File    \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502                         \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \n \u2502 \n \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502 ToolMessage Results  \u2502 \n \u2502 [                    \u2502 \n \u2502   \"Code: def api...\" \u2502 \n \u2502   \"\u2713 File written\"   \u2502 \n \u2502 ]                    \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500-\u2518 \n \u2502 \n \u25bc \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502 Return to LLM       \u2502 \n \u2502 (Continue workflow) \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Important : Tools can execute in parallel when they don't depend on each other, significantly improving performance for multi-file projects. \n Tool Categories in Our System \n Code Generation Tools : High-level abstractions \n generate_code : Planning and intent signaling \n create_project_structure : Multi-file orchestration \n analyze_code : Code understanding \n generate_tests : Test generation \n File Operation Tools : Low-level primitives \n write_file : Atomic file creation \n read_file : File content retrieval \n list_files : Directory exploration \n create_directory : Folder creation \n search_files : Pattern matching \n LangGraph Workflow: State Graphs Explained \n What is a StateGraph? \n LangGraph's StateGraph is a directed graph where nodes process state and edges route between nodes . Unlike linear chains, graphs support: \n \ud83d\udd04 Cycles : Nodes can loop back (e.g., tool_use \u2192 model_response \u2192 tool_use) \n \ud83c\udf3f Branching : Conditional edges route based on state \n \ud83d\udcbe State Accumulation : Each node updates shared state \n \u23f8\ufe0f Checkpointing : Pause and resume workflows \n Our Graph Structure \n workflow = StateGraph(AgentState) \n \n # Nodes: Functions that transform state \n workflow.add_node(\"model_response\", model_response_fn) \n workflow.add_node(\"tool_use\", tool_use_fn) \n \n # Unconditional edge: Always go from tool_use to model_response \n workflow.add_edge(\"tool_use\", \"model_response\") \n \n # Conditional edge: Route based on state \n workflow.add_conditional_edges( \n \"model_response\", \n check_tool_use,  # Router function \n { \n \"tool_use\": \"tool_use\",  # If tools requested \n END: END,  # If no tools \n } \n ) \n \n # Entry point \n workflow.set_entry_point(\"model_response\") \n \n # Compile into executable agent \n agent = workflow.compile(checkpointer=checkpointer) Execution Flow \n User Input: \"Create a FastAPI hello world app\" \n Step 1: model_response \n Input:  {\"messages\": [HumanMessage(\"Create a FastAPI hello world app\")]} \n Output: {\"messages\": [ \n HumanMessage(\"Create...\"), \n AIMessage(\"I'll create app.py...\", tool_calls=[{ \n \"name\": \"write_file\", \n \"args\": {\"file_path\": \"app.py\", \"content\": \"from fastapi...\"} \n }]) \n ]} \n \n Step 2: check_tool_use \n Decision: Has tool_calls \u2192 Route to \"tool_use\" \n \n Step 3: tool_use \n Input:  Previous messages + tool calls \n Action: Execute write_file(\"app.py\", \"from fastapi...\") \n Output: {\"messages\": [ \n ..., \n ToolMessage(\"\u2713 Successfully wrote to app.py\") \n ]} \n \n Step 4: model_response (cycle back) \n Input:  All previous messages + tool result \n Output: {\"messages\": [ \n ..., \n AIMessage(\"I've created your FastAPI app! Here's what I generated...\") \n ]} \n \n Step 5: check_tool_use \n Decision: No tool_calls \u2192 Route to END \n \n Final State: \n messages: [ \n HumanMessage(\"Create a FastAPI...\"), \n AIMessage(\"I'll create...\", tool_calls=[...]), \n ToolMessage(\"\u2713 Successfully wrote...\"), \n AIMessage(\"I've created your FastAPI app!...\") \n ] State Accumulation with add_messages \n messages: Annotated[Sequence[BaseMessage], add_messages] The add_messages annotation is a reducer that defines how state updates are merged: \n \ud83d\udcdd New messages append to existing list \n \ud83d\udd04 Messages with same ID update in place \n \ud83d\udcda Full conversation history maintained \n Checkpointing in Action \n Every node execution creates a checkpoint: \n Checkpoint 0: Initial state \n messages: [HumanMessage(\"Create...\")] \n \n Checkpoint 1: After model_response \n messages: [HumanMessage(...), AIMessage(..., tool_calls=[...])] \n \n Checkpoint 2: After tool_use \n messages: [..., ToolMessage(\"\u2713 Successfully wrote...\")] \n \n Checkpoint 3: After model_response (final) \n messages: [..., AIMessage(\"I've created...\")] You can resume from any checkpoint: \n # Resume from checkpoint 2 \n config = { \n \"configurable\": { \n \"thread_id\": \"session_123\", \n \"checkpoint_id\": \"checkpoint_2\" \n } \n } \n agent.invoke({\"messages\": []}, config) Key Design Patterns Used \n The implementation leverages several important software design patterns: \n 1. StateGraph Pattern (Workflow Orchestration) \n Purpose : Manage complex workflows with conditional routing \n Implementation : Three-node graph (model_response, tool_use, check_tool_use) \n Benefits : \n Clear separation between thinking and acting \n Easy to visualize and debug \n Extensible (add more nodes as needed) \n Supports cycles for iterative refinement \n 2. Tool Binding Pattern (Function Calling) \n Purpose : Enable LLM to invoke external functions \n Implementation : Tools bound to LLM via bind_tools() method \n Benefits : \n LLM intelligently chooses appropriate tools \n Type-safe parameter passing \n Automatic JSON schema generation \n Error handling at tool level \n 3. Checkpointing Pattern (State Persistence) \n Purpose : Persist conversation state across invocations \n Implementation : SQLite-based async checkpointing after each node \n Benefits : \n Resume interrupted conversations \n Time-travel debugging \n Audit trail for all interactions \n State inspection for analytics \n 4. Message Pattern (Communication Protocol) \n Purpose : Standardize all communication in the system \n Implementation : LangChain message types (Human, AI, System, Tool) \n Benefits : \n Type safety and validation \n Consistent serialization \n Easy to extend with metadata \n Works across all LangChain tools \n 5. Tool Registry Pattern (Plugin Architecture) \n Purpose : Dynamically manage available tools \n Implementation : Separate modules ( code_tools.py , file_tools.py ) \n Benefits : \n Modularity and separation of concerns \n Easy to add/remove tools \n Tools can be tested independently \n Clear organization \n 6. Reducer Pattern (State Accumulation) \n Purpose : Define how state updates are merged \n Implementation : add_messages reducer in AgentState \n Benefits : \n Automatic message deduplication \n Maintains conversation history \n Predictable state updates \n No manual state management \n 7. Async Context Manager Pattern (Resource Management) \n Purpose : Properly manage async resources (DB connections) \n Implementation : AsyncSqliteSaver with __aenter__ / __aexit__ \n Benefits : \n Guaranteed cleanup on exit \n Exception-safe resource handling \n Pythonic async code \n No resource leaks \n Pattern Interaction Example: \n # Pattern combination in action \n class CodeGeneratorAgent: \n def __init__(self): \n # Tool Registry Pattern \n self.tools = get_code_tools() + get_file_tools() \n \n # Tool Binding Pattern \n self.llm_with_tools = self.llm.bind_tools(self.tools) \n \n # StateGraph Pattern + Message Pattern \n workflow = StateGraph(AgentState) \n workflow.add_node(\"model_response\", self.model_response) \n \n async def initialize(self): \n # Async Context Manager + Checkpointing Pattern \n self._checkpointer_ctx = AsyncSqliteSaver.from_conn_string(db_path) \n self.checkpointer = await self._checkpointer_ctx.__aenter__() \n \n # Compile with all patterns integrated \n self.agent = workflow.compile(checkpointer=self.checkpointer) These patterns work together to create a robust, maintainable, and extensible code generation system. \n Benefits of This Architecture \n 1. Modularity and Extensibility \n \u2795 Easy to Add Tools : New capabilities = new @tool functions \n \ud83d\udd27 Swappable LLMs : Change from Claude to GPT-4 with one line \n \ud83c\udfa8 UI Flexibility : CLI, web, or mobile \u2014 same agent core \n 2. Robustness and Reliability \n \ud83d\udee1\ufe0f Error Recovery : Auto-fix mechanisms for common mistakes \n \u2705 Validation : Pydantic ensures type safety \n \ud83d\udd04 Retry Logic : Checkpoint-based error recovery \n \ud83d\udcca Observability : Full conversation logs for debugging \n 3. Scalability \n \ud83c\udf10 Multi-User Support : Isolated agent instances per session \n \u26a1 Async Throughout : FastAPI + asyncio for high concurrency \n \ud83d\udcbe Efficient State : SQLite handles thousands of conversations \n \ud83d\udce6 Stateless Workers : Deploy multiple app.py instances behind load balancer \n 4. Developer Experience \n \ud83c\udfa8 Beautiful CLI : Rich terminal output with markdown \n \ud83d\udd34 Live Updates : WebSocket streaming feels instantaneous \n \ud83d\udcdd Clear Errors : Helpful error messages guide LLM to correct itself \n \ud83c\udfaf Smart Defaults : Works out-of-the-box with sensible configuration \n 5. Production-Ready Features \n \ud83d\udd10 Environment Management : API keys in .env, never committed \n \ud83d\udcc1 Safe File Operations : Directory creation, path normalization \n \ud83e\uddf9 Resource Cleanup : Proper async context manager usage \n \ud83c\udfe5 Health Checks : /health endpoint for monitoring \n Real-World Use Cases \n 1. Rapid Prototyping \n User: \"Create a microservice for user authentication with JWT\" \n Agent: \n \u2713 Generated auth.py (authentication logic) \n \u2713 Generated models.py (user models) \n \u2713 Generated main.py (FastAPI app) \n \u2713 Generated requirements.txt \n \u2713 Generated README.md 2. Learning and Education \n User: \"Explain how binary search works and implement it in Python\" \n Agent: \n \u2713 Generated explanation.md (algorithm explanation with visuals) \n \u2713 Generated binary_search.py (implementation with comments) \n \u2713 Generated test_binary_search.py (unit tests) 3. Code Migration \n User: \"Here's my Flask app. Convert it to FastAPI.\" \n Agent: \n 1. Analyzes Flask code with analyze_code tool \n 2. Generates equivalent FastAPI code \n 3. Writes converted files \n 4. Creates migration guide 4. Testing and Quality \n User: \"Generate tests for data_processing.py\" \n Agent: \n 1. Reads existing code with read_file \n 2. Analyzes functions and edge cases \n 3. Generates comprehensive pytest suite Code Workflow Example: Complete Execution \n Let's walk through a real interaction to see all components working together: \n User Request (CLI): \n \ud83d\udcac What code would you like to generate? \n > Create a FastAPI REST API for a todo list with SQLite Execution Trace: \n \ud83e\udd14 Generating code... \n \n [model_response node] \n \u251c\u2500 System message added (instructions, rules, workflow) \n \u251c\u2500 User message: \"Create a FastAPI REST API...\" \n \u251c\u2500 LLM invoked with tools bound \n \u2514\u2500 Response: \n \u251c\u2500 Content: \"I'll create a complete FastAPI todo list API...\" \n \u2514\u2500 Tool calls: \n \u251c\u2500 write_file(file_path=\"./generated_code/main.py\", content=\"from fastapi...\") \n \u251c\u2500 write_file(file_path=\"./generated_code/models.py\", content=\"from pydantic...\") \n \u2514\u2500 write_file(file_path=\"./generated_code/database.py\", content=\"import sqlite3...\") \n \n [check_tool_use edge] \n \u2514\u2500 Decision: Has 3 tool calls \u2192 Route to \"tool_use\" \n \n [tool_use node] \n \ud83d\udd27 Executing tool: write_file \n Arguments: ['file_path', 'content'] \n \u2713 Successfully wrote 1247 characters to .../main.py \n \n \ud83d\udd27 Executing tool: write_file \n Arguments: ['file_path', 'content'] \n \u2713 Successfully wrote 892 characters to .../models.py \n \n \ud83d\udd27 Executing tool: write_file \n Arguments: ['file_path', 'content'] \n \u2713 Successfully wrote 654 characters to .../database.py \n \n [model_response node - cycle 2] \n \u251c\u2500 Input: All previous messages + 3 tool results \n \u251c\u2500 LLM sees successful file creation \n \u2514\u2500 Response: \n \u2514\u2500 Content: \"I've created your FastAPI todo list API with 3 files: \n 1. main.py - FastAPI app with CRUD endpoints \n 2. models.py - Pydantic models for todos \n 3. database.py - SQLite database setup \n \n To run: uvicorn main:app --reload\" \n \u2514\u2500 No tool calls \n \n [check_tool_use edge] \n \u2514\u2500 Decision: No tool calls \u2192 Route to END \n \n [END] \n Final output displayed to user with rich formatting Checkpoints Created: \n checkpoint_1.db: Initial user message \n checkpoint_2.db: LLM response with tool calls \n checkpoint_3.db: After tool execution \n checkpoint_4.db: Final LLM response Press enter or click to view image in full size \n \n Press enter or click to view image in full size \n \n Press enter or click to view image in full size \n \n Press enter or click to view image in full size \n \n User Request (UI): \n (Code_editor) C:\\Users\\nayak\\Documents\\Code_editor>python app.py \n INFO:     Will watch for changes in these directories: ['C:\\\\Users\\\\nayak\\\\Documents\\\\Code_editor'] \n INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit) \n INFO:     Started reloader process [24204] using WatchFiles \n INFO:     Started server process [3700] \n INFO:     Waiting for application startup. \n \ud83d\ude80 Starting AI Code Generator Web Application... \n \ud83d\udcc1 Output directory: C:\\Users\\nayak\\Documents\\Code_editor\\generated_code \n INFO:     Application startup complete. \n INFO:     127.0.0.1:52513 - \"GET / HTTP/1.1\" 200 OK \n INFO:     127.0.0.1:52513 - \"GET /static/styles.css HTTP/1.1\" 304 Not Modified \n INFO:     127.0.0.1:49906 - \"GET /static/app.js HTTP/1.1\" 304 Not Modified \n INFO:     127.0.0.1:49906 - \"GET /api/tools HTTP/1.1\" 200 OK \n INFO:     127.0.0.1:50916 - \"WebSocket /ws/session_1762688649849_ogz6vwyep\" [accepted] \n \n \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \n \u2551                                                           \u2551 \n \u2551   \ud83d\ude80  AI CODE GENERATOR  \ud83d\ude80                              \u2551 \n \u2551                                                           \u2551 \n \u2551   \u25b8 Powered by Claude Sonnet 4.5 + LangGraph            \u2551 \n \u2551   \u25b8 Generate code from natural language                 \u2551 \n \u2551   \u25b8 Type 'exit' or 'quit' to terminate                  \u2551 \n \u2551                                                           \u2551 \n \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \n \n \ud83d\udd27 Loading tools... \n \u2713 Loaded 5 code generation tools \n \u2713 Loaded 5 file operation tools \n INFO:     connection open \n WARNING:  WatchFiles detected changes in 'generated_code\\dice_server\\server.py'. Reloading... \n INFO:     Shutting down \n INFO:     connection closed \n INFO:     Waiting for background tasks to complete. (CTRL+C to force quit) \n WebSocket error: Cannot call \"send\" once a close message has been sent. \n INFO:     Waiting for application shutdown. \n \ud83d\uded1 Shutting down... \n INFO:     Application shutdown complete. \n INFO:     Finished server process [3700] \n INFO:     Started server process [34836] \n INFO:     Waiting for application startup. \n \ud83d\ude80 Starting AI Code Generator Web Application... \n \ud83d\udcc1 Output directory: C:\\Users\\nayak\\Documents\\Code_editor\\generated_code \n INFO:     Application startup complete. \n INFO:     127.0.0.1:57033 - \"WebSocket /ws/session_1762688649849_ogz6vwyep\" [accepted] \n \n \u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557 \n \u2551                                                           \u2551 \n \u2551   \ud83d\ude80  AI CODE GENERATOR  \ud83d\ude80                              \u2551 \n \u2551                                                           \u2551 \n \u2551   \u25b8 Powered by Claude Sonnet 4.5 + LangGraph              \u2551 \n \u2551   \u25b8 Generate code from natural language                   \u2551 \n \u2551   \u25b8 Type 'exit' or 'quit' to terminate                    \u2551 \n \u2551                                                           \u2551 \n \u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d \n \n \ud83d\udd27 Loading tools... \n \u2713 Loaded 5 code generation tools \n \u2713 Loaded 5 file operation tools \n INFO:     connection open \n INFO:     connection closed \n INFO:     127.0.0.1:52134 - \"WebSocket /ws/session_1762688649849_ogz6vwyep\" [accepted] \n INFO:     connection open \n INFO:     connection closed \n INFO:     127.0.0.1:54107 - \"WebSocket /ws/session_1762688649849_ogz6vwyep\" [accepted] \n INFO:     connection open \n INFO:     connection closed \n INFO:     127.0.0.1:62643 - \"WebSocket /ws/session_1762688649849_ogz6vwyep\" [accepted] \n INFO:     connection open Press enter or click to view image in full size \n \n Press enter or click to view image in full size \n \n \n sample generated Code by the coding assiatant \n \"\"\" \n Dice Rolling Server using FastMCP \n A simple MCP server that provides dice rolling functionality \n \"\"\" \n \n import random \n from typing import Optional \n from mcp.server.fastmcp import FastMCP \n \n # Initialize FastMCP server \n mcp = FastMCP(\"Dice Roller\") \n \n \n @mcp.tool() \n def roll_dice(sides: int = 6, count: int = 1) -> dict: \n \"\"\" \n Roll one or more dice with a specified number of sides. \n  \n Args: \n sides: Number of sides on each die (default: 6) \n count: Number of dice to roll (default: 1) \n  \n Returns: \n Dictionary containing the rolls and their sum \n \"\"\" \n if sides < 2: \n return {\"error\": \"Dice must have at least 2 sides\"} \n  \n if count < 1 or count > 100: \n return {\"error\": \"Must roll between 1 and 100 dice\"} \n  \n rolls = [random.randint(1, sides) for _ in range(count)] \n  \n return { \n \"rolls\": rolls, \n \"total\": sum(rolls), \n \"count\": count, \n \"sides\": sides, \n \"average\": round(sum(rolls) / len(rolls), 2) \n } \n \n \n @mcp.tool() \n def roll_d20(advantage: bool = False, disadvantage: bool = False) -> dict: \n \"\"\" \n Roll a d20 (20-sided die), commonly used in tabletop RPGs. \n  \n Args: \n advantage: Roll twice and take the higher result \n disadvantage: Roll twice and take the lower result \n  \n Returns: \n Dictionary containing the roll result(s) \n \"\"\" \n if advantage and disadvantage: \n return {\"error\": \"Cannot have both advantage and disadvantage\"} \n  \n roll1 = random.randint(1, 20) \n  \n if not advantage and not disadvantage: \n return { \n \"roll\": roll1, \n \"type\": \"normal\", \n \"critical_hit\": roll1 == 20, \n \"critical_fail\": roll1 == 1 \n } \n  \n roll2 = random.randint(1, 20) \n  \n if advantage: \n result = max(roll1, roll2) \n roll_type = \"advantage\" \n else: \n result = min(roll1, roll2) \n roll_type = \"disadvantage\" \n  \n return { \n \"roll\": result, \n \"rolls\": [roll1, roll2], \n \"type\": roll_type, \n \"critical_hit\": result == 20, \n \"critical_fail\": result == 1 \n } \n \n \n @mcp.tool() \n def roll_custom(dice_notation: str) -> dict: \n \"\"\" \n Roll dice using standard dice notation (e.g., \"2d6+3\", \"1d20\", \"3d8-2\"). \n  \n Args: \n dice_notation: Dice notation string (format: XdY+Z where X is count, Y is sides, Z is modifier) \n  \n Returns: \n Dictionary containing the rolls and total \n \"\"\" \n try: \n # Parse dice notation \n dice_notation = dice_notation.lower().replace(\" \", \"\") \n  \n # Handle modifier \n modifier = 0 \n if \"+\" in dice_notation: \n dice_part, mod_part = dice_notation.split(\"+\") \n modifier = int(mod_part) \n elif \"-\" in dice_notation and dice_notation.count(\"-\") == 1: \n dice_part, mod_part = dice_notation.split(\"-\") \n modifier = -int(mod_part) \n else: \n dice_part = dice_notation \n  \n # Parse dice \n if \"d\" not in dice_part: \n return {\"error\": \"Invalid dice notation. Use format like '2d6' or '1d20+5'\"} \n  \n count_str, sides_str = dice_part.split(\"d\") \n count = int(count_str) if count_str else 1 \n sides = int(sides_str) \n  \n if sides < 2: \n return {\"error\": \"Dice must have at least 2 sides\"} \n  \n if count < 1 or count > 100: \n return {\"error\": \"Must roll between 1 and 100 dice\"} \n  \n # Roll the dice \n rolls = [random.randint(1, sides) for _ in range(count)] \n total = sum(rolls) + modifier \n  \n return { \n \"notation\": dice_notation, \n \"rolls\": rolls, \n \"modifier\": modifier, \n \"subtotal\": sum(rolls), \n \"total\": total, \n \"count\": count, \n \"sides\": sides \n } \n  \n except Exception as e: \n return {\"error\": f\"Invalid dice notation: {str(e)}\"} \n \n \n @mcp.tool() \n def roll_stats() -> dict: \n \"\"\" \n Roll character stats for tabletop RPGs (rolls 4d6, drops lowest, 6 times). \n  \n Returns: \n Dictionary containing six ability scores \n \"\"\" \n stats = [] \n all_rolls = [] \n  \n for _ in range(6): \n rolls = [random.randint(1, 6) for _ in range(4)] \n rolls.sort() \n dropped = rolls[0] \n kept = rolls[1:] \n stat_value = sum(kept) \n  \n stats.append(stat_value) \n all_rolls.append({ \n \"rolls\": rolls, \n \"dropped\": dropped, \n \"kept\": kept, \n \"value\": stat_value \n }) \n  \n return { \n \"stats\": stats, \n \"total\": sum(stats), \n \"average\": round(sum(stats) / 6, 2), \n \"details\": all_rolls \n } \n \n \n @mcp.tool() \n def coin_flip(count: int = 1) -> dict: \n \"\"\" \n Flip one or more coins. \n  \n Args: \n count: Number of coins to flip (default: 1) \n  \n Returns: \n Dictionary containing flip results \n \"\"\" \n if count < 1 or count > 1000: \n return {\"error\": \"Must flip between 1 and 1000 coins\"} \n  \n flips = [random.choice([\"Heads\", \"Tails\"]) for _ in range(count)] \n heads_count = flips.count(\"Heads\") \n tails_count = flips.count(\"Tails\") \n  \n return { \n \"flips\": flips, \n \"count\": count, \n \"heads\": heads_count, \n \"tails\": tails_count, \n \"heads_percentage\": round((heads_count / count) * 100, 2) \n } \n \n \n if __name__ == \"__main__\": \n # Run the server \n mcp.run() CLI \u2014 main.py Architecture Compliance \n The main.py file correctly follows the architecture described in ARCHITECTURE.md: \n 1. \u2705 Environment Setup \n 2. \u2705 Agent Initialization \n 3. \u2705 Workflow Execution \n 4. \u2705 Resource Cleanup \n Architecture Components Used \n \u25cf LangGraph StateGraph: \u2705 Used in `CodeGeneratorAgent` \n \u25cf SQLite Checkpointing: \u2705 Initialized in `agent.initialize()` \n \u25cf Tool Integration: \u2705 Code tools + File tools loaded \n \u25cf Claude Sonnet 4.5: \u2705 Configured in agent initialization \n \u25cf Async/Await: \u2705 Properly used throughout \n \ud83d\ude80 FastAPI Web Application (app.py) \n Overview \n The full-stack web application that provides the same functionality as main.py but with a modern web interface. \n Features \n 1. REST API Endpoints \n 2. WebSocket Support \n 3. Beautiful Web UI \n Architecture \n The FastAPI app follows the same architecture principles: \n \u25cf Agent Manager: Manages multiple agent instances for concurrent users \n \u25cf Session-based: Each user gets a unique session with persistent state \n \u25cf Same Core Agent: Uses the same `CodeGeneratorAgent` class \n \u25cf Silent Console: Suppresses terminal output for web use \n \u25cf WebSocket Streaming: Real-time updates during code generation \n Differences Web UI (app.py) between CLI (main.py) \n \n \n Press enter or click to view image in full size \n \n \n \n \n \ud83d\udcdd Notes \n \u25cf Both `main.py` and `app.py` use the same core `CodeGeneratorAgent` \n \u25cf The architecture is consistent across both implementations \n \u25cf Web version adds session management for concurrent users \n \u25cf All tools and capabilities are available in both versions \n Future Improvements \n 1. Enhanced Tool Ecosystem \n \ud83d\udc33 Docker Integration : Generate and build Docker containers \n \ud83e\uddea Test Execution : Run tests and fix failures automatically \n \ud83d\udce6 Package Management : Auto-install dependencies \n \ud83d\udd0d Code Search : Semantic code search across projects \n \ud83c\udf10 API Testing : Generate and execute API tests \n 2. Advanced Workflows \n \ud83d\udd04 Multi-Step Planning : Break complex projects into subtasks \n \ud83e\udd1d Multi-Agent Collaboration : Specialist agents (frontend, backend, testing) \n \ud83c\udfaf Goal-Oriented Execution : Define end goals, let agent plan approach \n \ud83e\udde0 Learning from Feedback : Improve based on user corrections \n 3. UI/UX Enhancements \n \ud83c\udfa8 React Frontend : Modern UI with code highlighting \n \ud83d\udcf1 Mobile App : iOS/Android code generation on the go \n \ud83c\udfa4 Voice Input : Speak your code requirements \n \ud83d\udc41\ufe0f Visual Programming : Drag-and-drop project structure design \n 4. Enterprise Features \n \ud83d\udc65 Team Collaboration : Shared projects and sessions \n \ud83d\udd10 Access Control : Role-based permissions \n \ud83d\udcca Analytics Dashboard : Usage stats, popular patterns \n \ud83d\udcbc Custom Templates : Organization-specific code templates \n \ud83d\udd0c IDE Plugins : VSCode, JetBrains integration \n 5. Performance Optimizations \n \u26a1 Caching : Cache common code patterns \n \ud83d\ude80 Streaming Optimization : Faster token streaming \n \ud83d\udcbe PostgreSQL Backend : Scale beyond SQLite \n \ud83c\udf0d CDN Integration : Serve static assets globally \n \ud83d\udd25 Redis Sessions : Distributed session management \n 6. Quality Improvements \n \u2705 Code Linting : Auto-format with black, prettier \n \ud83d\udee1\ufe0f Security Scanning : Detect vulnerabilities in generated code \n \ud83d\udcdd Documentation Generation : Auto-generate API docs \n \ud83e\uddea Coverage Reports : Ensure test coverage \n \ud83d\udcc8 Performance Profiling : Optimize generated code \n 7. Scalability Enhancements \n Current architecture supports single-user CLI or multi-session web. To scale further: \n \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \n \u2502  Current Architecture: Async + SQLite                   \u2502 \n \u2502  \u2022 One agent per web session                            \u2502 \n \u2502  \u2022 Thread-based state isolation                         \u2502 \n \u2502  \u2022 SQLite for persistence (local)                       \u2502 \n \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \n \u2502  Horizontal Scaling Path:                               \u2502 \n \u2502  1. Replace SQLite \u2192 PostgreSQL (distributed state)     \u2502 \n \u2502  2. Add Redis for session management                    \u2502 \n \u2502  3. Deploy multiple FastAPI instances                   \u2502 \n \u2502  4. Use load balancer (NGINX/AWS ALB)                   \u2502 \n \u2502  5. Implement request queuing (Celery/RQ)               \u2502 \n \u2502  6. Add caching layer (Redis) for common patterns       \u2502 \n \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \n \u2502  Expected Throughput:                                    \u2502 \n \u2502  \u2022 Current: 1-10 concurrent users                       \u2502 \n \u2502  \u2022 With PostgreSQL + Redis: 100-1000 users              \u2502 \n \u2502  \u2022 With multi-instance + queue: 10,000+ users           \u2502 \n \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Migration Example: \n # Replace AsyncSqliteSaver with PostgreSQL \n from langgraph.checkpoint.postgres import AsyncPostgresSaver \n \n checkpointer = await AsyncPostgresSaver.from_conn_string( \n \"postgresql://user:pass@host/db\" \n ) \n \n # Add Redis \n[truncated]",
  "title": "Mimicking Claude Code CLI Using LangGraph, Anthropic, and FastAPI | by Plaban Nayak | Level Up Coding",
  "content_type": "text/html; charset=utf-8"
}