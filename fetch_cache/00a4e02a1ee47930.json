{
  "url": "https://arxiv.org/html/2509.23586v1",
  "type": "webpage",
  "content": "    Improving the Efficiency of LLM Agent Systems through Trajectory Reduction           \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n     Improving the Efficiency of LLM Agent Systems through Trajectory Reduction \n    Yuan-An Xiao  xiaoyuanan@pku.edu.cn   Peking University China   ,  Pengfei Gao  gaopengfei.se@bytedance.com   ByteDance China   ,  Chao Peng  pengchao.x@bytedance.com   ByteDance China   and  Yingfei Xiong  xiongyf@pku.edu.cn   Peking University China   \n  (2018) \n   Abstract. \n  Multi-turn agent systems based on Large Language Models (LLMs) have been increasingly popular for software engineering tasks. While LLM agents show decent effectiveness, the high computational cost of input tokens due to the ever-growing trajectory remains an efficiency concern for their applications. Efficiency is largely neglected in existing studies and agent products, and this paper fills the gap by introducing an inference-time trajectory reduction approach to reduce the cost of agents. \n  Through analyzing existing agent trajectories, we demonstrate that useless, redundant, and expired information is widespread in all trajectories, which can be identified and reduced without harming the agent\u2019s performance. We then design a simple yet effective trajectory reduction approach, AgentDiet, which automatically removes such waste information. We implement AgentDiet on a top-performing coding agent, and the evaluation on two LLMs and two benchmarks shows that AgentDiet can reduce input tokens by 39.9% \u223c \\sim 59.7%, or the final computational cost by 21.1% \u223c \\sim 35.9%, while maintaining the same agent performance. This indicates that trajectory reduction is a promising direction for agent systems. \n  \n  \u2020 \u2020 copyright: acmlicensed \u2020 \u2020 journalyear: 2018 \u2020 \u2020 doi: XXXXXXX.XXXXXXX \u2020 \u2020 conference: Make sure to enter the correct\nconference title from your rights confirmation email; June 03\u201305,\n2018; Woodstock, NY \u2020 \u2020 isbn: 978-1-4503-XXXX-X/18/06    1. Introduction \n   Large Language Models (LLMs) have been widely used for various software engineering tasks such as code generation, testing, and repair (Liu et\u00a0al., 2024 ; Jiang et\u00a0al., 2023 ; Farzandway and Ghassemi, 2025 ; Ahmed et\u00a0al., 2024 ; Xia et\u00a0al., 2024 ; Xia and Zhang, 2024 ) . The most recent trend in the application of LLMs is agent-based approaches (Bouzenia et\u00a0al., 2024 ; Qin et\u00a0al., 2024 ; Kang et\u00a0al., 2025 ; Wang et\u00a0al., 2024c ; Yang et\u00a0al., 2024a ) , where LLMs are asked to complete difficult tasks through multi-step reasoning and tool calling. For example, mini-SWE-agent (Yang et\u00a0al., 2024a ) achieves a single-shot performance of 65% in SWE-bench Verified (Jimenez et\u00a0al., 2024 ) using Claude 4 Sonnet (Anthropic, 2025b ) , indicating that recent agents can fix real-world GitHub issues with basic tools such as file editing and bash scripting. Given their satisfactory performance, LLM agents are integrated in various AI products (Anthropic, 2025a ; Cursor, 2025 ) , and 24% professional developers already use LLM agents daily or weekly (Overflow, 2025 ) . \n  \n   Although these agent approaches demonstrate decent performance in terms of effectiveness, efficiency remains a concern for their application. In a recent survey by StackOverflow (Overflow, 2025 ) , 53% of the participants believe that the cost of using AI agents is a barrier for them. The efficiency issue is largely neglected in existing studies, leaving a large gap to address. \n  \n   The root cause of the efficiency issue comes from the workflow of LLM agents, i.e., the ever-growing trajectory. In a typical agent system, once the LLM calls a tool, the tool call and its result are kept in the trajectory until the agent finishes the whole task. Therefore, if the LLM opens a large file or runs a command that generates verbose output, the computational cost of these trajectory tokens snowballs for each of the following steps (Fan et\u00a0al., 2025 ) , resulting in a large waste. In fact, daily usage of the Claude 4 Sonnet reaches 100B tokens on the OpenRouter platform (OpenRouter, 2025 ) , and 99% of them are input tokens accumulated in the trajectory, while only 1% tokens are generated by the LLM, showing great potential for improvement. \n  \n   To address the above efficiency issue, this paper focuses on reducing trajectory tokens for coding LLM agents. In academia, most existing papers on token reduction focus on single-turn tasks, such as trimming the retrieved input for question-answering (Pan et\u00a0al., 2024 ; Chirkova et\u00a0al., 2025 ; Wang et\u00a0al., 2023 ; Louis et\u00a0al., 2025 ; Zhang et\u00a0al., 2022 ) , which are different with coding agents in multiple aspects: (1) these approaches reduce all input tokens at once, while trajectory tokens gradually build up in the agents, and the timing of reduction is a new research question; (2) these approaches reduce tokens in natural language, while coding agents process structured information such as software source code; (3) some approaches require modifying the process of LLM inference, while the best performing LLMs on code are proprietary LLMs that do not provide such capabilities. To the best of our knowledge, there is no existing publication on the inference-time trajectory reduction for coding agents. In the industry, some agent products (Anthropic, 2025a ; Cursor, 2025 ) sparingly apply LLM-based compression only when the context window is full, focusing on robustness rather than efficiency. As a result, the potential for efficiency improvement by trajectory reduction remains unknown. This paper reveals this potential by designing an LLM-based trajectory reduction approach and evaluating it on multiple benchmarks and LLMs. \n  \n   In Section 2 , we analyze the problem and investigate the possibility of cost reduction, revealing that the trajectories of LLM agents contain a large amount of waste, including useless, redundant, or expired information. We then propose the design of a prototype trajectory reduction approach. In Section 3 , we instantiate the design into a concrete algorithm, AgentDiet. Section 4 discusses the integration of AgentDiet in agents and the effect of hyperparameter settings through a quantitative experiment. Section 5 evaluates AgentDiet on two LLMs and two benchmarks, showing that it can steadily reduce the input tokens by 39.9% \u223c \\sim 59.7%, or final computation cost by 21.1% \u223c \\sim 35.9%, while maintaining the same agent performance. \n  \n   The contributions of this paper are summarized as follows: \n  \n     \u2022   We reveal that inference-time trajectory reduction of coding agents is a promising new direction, with preliminary results showing that significant cost reduction is possible without harming agent performance; \n  \n  \n   \u2022   We propose AgentDiet, a simple but effective trajectory reduction approach, which is open-source and can be easily integrated in different coding agents; \n  \n  \n   \u2022   We discuss the design of trajectory reduction through case studies and large-scale quantitative experiments. \n  \n  \n   \n     2. Problem Analysis \n   In Section 2.1 , we first discuss preliminary concepts related to this paper, i.e., the typical workflow of LLM agents and ingredients in their trajectories. We then motivate our approach in Section 2.2 and Section 2.3 by gradually analyzing the efficiency problem in the trajectory. In particular, we will answer a series of questions: \n  \n     \u2022   Is there waste in the trajectories? (Yes, there is a lot.) \n  \n  \n   \u2022   Can we automatically identify and reduce the waste in trajectories? (Yes, it is possible to use LLM for trajectory reduction.) \n  \n  \n   \u2022   Do LLMs know when to perform trajectory reduction on their own? (No, so we enforce the trajectory reduction step with a separate reflection module.) \n  \n  \n   \u2022   Is it ideal and efficient to call an LLM for that purpose? (Not really, but the overhead is under control with optimizations.) \n  \n  \n   \n    2.1. Preliminaries \n    \u201cAgents are models using tools in a loop.\u201d \n   - Hannah Moran from Anthropic, Prompting for Agents \n  \n   Figure 1. Typical workflow of an LLM agent.    In recent years, Large Language Models (LLMs) have shown the ability to assist with various software engineering tasks. Following established approaches like Chain-of-Thought (Wei et\u00a0al., 2022 ) , ReAct (Yao et\u00a0al., 2023 ) , and CodeAct (Wang et\u00a0al., 2024a ) , the LLM agent has been a promising paradigm for integrating LLMs into such tasks by equipping the LLM with tools to interact with the environment. In the field of software engineering, this means the LLM can operate on the codebase with the ability to read files, make changes, execute commands, etc. \n  \n   Fig. 1 shows the typical workflow of an LLM agent. The core of an agent is its trajectory , which records all the necessary context that will be given to the LLM. An agent starts with an initial trajectory that contains the instruction to the LLM (in a system message) and the current task (in a user message). Then, the LLM analyzes the current situation and predicts the next action to take in a standard tool call format (in an assistant message). The agent system parses the generated tool call, performs the action, and returns the output to the LLM (in a tool message). In this loop, assistant and tool messages are continuously concatenated into the trajectory, which will be the input of the subsequent LLM predictions. The loop continues until the LLM decides to finish the task with a special tool call, or when it reaches the step limit. \n  \n   From this workflow, we can identify an obvious efficiency issue in such agent systems: Once the agent calls a tool, the corresponding assistant and tool messages will be concatenated into the trajectory and will be kept forever until the task finishes. A long message will be included in each of the subsequent LLM predictions, even if the content is not (or no longer) relevant to the task, causing an inefficient use of computational power. As a quantitative result, the average trajectory to solve a single GitHub issue, as we collected from the SWE-bench Verified (Jimenez et\u00a0al., 2024 ) benchmark, contains 48.4K tokens in 40 steps. Breaking down these tokens, tool messages (containing the results of tool calls) use 30.4K tokens, assistant messages use 13.7K tokens (where 11.9K among them are the arguments of tool calls), and system/user messages (containing the initial instructions) use 4.4K tokens. Since each token concatenated into the trajectory is included in every subsequent input to the LLM, the accumulated token usage per issue reaches 1.0M, which requires excessive computational resources. \n  \n   Recent LLMs are equipped with the KV Cache mechanism, which mitigates the issue of high computational cost by caching repeated calculation of the Key and Value matrices in the Transformer (Vaswani et\u00a0al., 2017 ) architecture. However, this does not eliminate the need for trajectory reduction because: (1) KV Cache caches only part of the computations, but the remaining calculations are still costly as the trajectory grows; (2) KV Cache takes hardware resources like VRAM and I/O bandwidth, so reducing the length of the trajectory saves such valuable resources. Actually, the existence of KV Cache makes the design of trajectory reduction approaches a more challenging research question, since modifying a token in the trajectory invalidates the cache for all following tokens. \n  \n   In addition to harming efficiency, long messages in the context window also cause the performance of LLMs to degrade (Liu et\u00a0al., 2023 ; Li et\u00a0al., 2025 ) . Therefore, reducing the length of the trajectories does not necessarily lead to a drop in the agent\u2019s performance. Instead, it has the potential to slightly improve the performance by removing waste from the trajectories. \n  \n     2.2. Waste in Trajectories \n   In this subsection, we begin to analyze the possibility of trajectory reduction by first identifying typical waste in trajectories. For this purpose, we refer to SWE-bench Verified (Jimenez et\u00a0al., 2024 ) , a popular benchmark that requires agent systems to solve GitHub issues in popular Python repositories. The benchmark provides the logs and trajectories of all participants for public download. We downloaded and manually inspected the trajectories of Trae Agent (Gao et\u00a0al., 2025 ) , which is based on Claude 4 Sonnet and ranked top on SWE-bench Verified. \n  \n   By qualitatively inspecting the contents of the trajectories, we found that the waste is widespread in almost all trajectories. We then categorized them into three typical scenarios in which tokens could be removed or compressed from a human perspective: \n  \n    2.2.1. Useless information \n   Some information is not relevant to the task and can be safely removed with minimal information loss. For example, nearly all trajectories begin with a tool call to enumerate all files in the repository, and the tool response includes cache and resource files (e.g., files under the __pycache__ and .egg-info directories). Also in all trajectories, the agent executes commands to build and test the project, which may generate verbose output (e.g., GNU make prints the message \u201c make[2]: Entering/Leaving directory '...' \u201d for each visited location by default). Although Trae Agent has a mechanism to truncate the output to the first 16KB, a fixed threshold only avoids the extreme case, and we still frequently observe useless information within this threshold. \n  \n     2.2.2. Redundant information \n   When a piece of information appears multiple times in the trajectory, redundant copies can be removed without losing information. The most typical case of redundant information is the str_replace_editor tool, which is a standard file editing tool designed by Anthropic and frequently used in all trajectories. The tool call arguments for str_replace_editor are passed as JSON, such as {\"command\": \"str_replace\", \"path\": \"F\", \"old_str\": \"P\", \"new_str\": \"Q\"} , where P is a unique fragment of file F, typically containing multiple lines, which will be replaced by Q. The tool will perform the string replacement and then respond with the replaced result. There are multiple redundancies in such a tool call: (1) Q in the argument repeats with the replaced result in the response; (2) Q may contain same ingredients (e.g., shared statements) in P; (3) the agent should have retrieved the fragment to edit prior to this tool call, so P repeats with some code in previous steps in the trajectory. \n  \n     2.2.3. Expired information \n   Information relevant to a step may no longer be necessary after the step is completed. The most typical case is where the agent loops through many subjects to find the relevant one. For example, in the process of diagnosing the root cause of the issue, the agent often searches for a symbol with the grep shell command, and then opens each file to read as occurred in the search result. This process takes a few steps in the trajectory, and after the agent identifies the faulty file, most of the content in other files is no longer useful. \n  \n      2.3. A Prototype Approach to Trajectory Reduction \n    2.3.1. Identifying the waste \n   In this subsection, we discuss how to automatically identify and reduce the amount of waste in trajectories. A straightforward idea will be to maintain a set of rules (such as regular expressions) for each typical kind of waste, but such rules are not likely to cover a variety of scenarios. For example, it is difficult to write a rule to identify all verbose output in test cases, since every project has its own output format. \n  \n   Figure 2. A case study of reducing information waste in the trajectory by GPT-5 mini and LLMLingua-2.    Given the heuristic nature of this task, we decided to identify and reduce waste with the help of an LLM. As a case study, we picked a typical example of the \u201cuseless information\u201d category, where the test command shows the list of each test passed in a verbose way, as shown in Fig. 2 . We designed a prompt based on LLMLingua-2 (Pan et\u00a0al., 2024 ) , a recent work on prompt compression, instructing GPT-5 mini to replace useless, redundant, or expired information with a placeholder of \u201c\u2026 (a short description)\u201d. The full prompt is available in the artifact, and we omit it here due to space constraints. On the right side of Fig. 2 , we can see that GPT-5 mini successfully removes these lines with a description (\u201cindividual test lines omitted; mostly PASSED\u201d), while preserving the name of the failed test at the end, which is important for the task. \n  \n   We also show the compression result of LLMLingua-2 in this case, at the bottom of Fig. 2 . Its result is rather uninformative because the names of the test cases are corrupted in the middle. The main reason for the huge difference between GPT-5 mini and LLMLingua-2 is that LLMLingua-2 is a small model (based on xlm-roberta-large  (Ruder et\u00a0al., 2019 ) ) distilled from synthesized data on natural language. While such a model is trained to remove redundancy in the natural language at a token granularity, it may not have the reasoning ability at a larger granularity (e.g., the full list of passed tests is useless for the task). Therefore, LLMs such as GPT-5 mini seem like a promising choice for the trajectory reduction task. \n  \n     2.3.2. The timing of trajectory reduction \n   After showing that trajectory reduction is possible with an LLM, the next natural topic is the timing of trajectory reduction. \n  \n   Ideally, if the agent system is powerful enough, it can reduce its own trajectory given a tool for that purpose. We then experiment with this hypothesis by implementing a tool called erase , where the agent can override the existing steps in the trajectory. We assign a numeric ID to each segment in the trajectory, and the LLM can call the erase tool with arguments such as {\"id\": 17, \"takeaway\": \"unrelated content\"} to replace the segment with a short takeaway. However, based on our observation, even with the most powerful models such as Claude 4 Sonnet, the agent often persists on the original task without calling erase under explicit prompts. Below is an example collected on the django__django-13012 instance in SWE-bench Verified, where the agent continues on the task even if asked explicitly for trajectory reduction: \n  \n     A failed example (django__django-13012)    [System] \u2026 The user will request for reflection on previous steps by saying \u201d#reflection\u201d. In that case, you enter reflection mode and may call the \u201derase\u201d tool to modify previous steps. If you believe it is beneficial to modify some steps, call \u201derase\u201d and DO NOT call other tools. Otherwise, DO NOT continue the task and DO NOT call any tool.  [User] Problem statement: \u2026  [Assistant] \u2026 [Tool] \u2026 [Assistant] \u2026 [Tool] \u2026 [Assistant] \u2026 [Tool] \u2026  (existing steps in the trajectory)  [User] #reflection. IMPORTANT: Unless I tell you to leave, ONLY call the \u201derase\u201d tool and DO NOT continue working on the original task.  [Assistant] I\u2019m examining the Django codebase to find how ExpressionWrapper and GROUP BY clauses interact. I\u2019ve looked at multiple relevant code sections, but I haven\u2019t yet found the specific \u201dget_group_by_cols\u201d method in the Expression base class. Let me continue looking for this method since it\u2019s critical to understanding how constants are excluded from GROUP BY clauses. Let me continue by examining more of the Expression class code to find the relevant \u201dget_group_by_cols\u201d method: \u2026 (calling other tools)   \n   A possible reason is that the LLM memorized the standard procedure of program repair during training, so it has an uncontrollable tendency to follow that procedure. Therefore, it requires careful fine-tuning to allow the agent to reduce its own trajectory, which is resource-intensive, error-prone, and often impossible for proprietary LLMs. \n  \n   To overcome this problem, we take a step back and move the trajectory reduction step to a separate module called reflection module . This LLM-based module will be explicitly called by the outer system to reduce the trajectory when there is an estimated benefit. In this way, the timing of the trajectory reduction is controlled by an external system, and the LLM that runs the agent is unaware of the trajectory reduction, reducing the disturbance to its original workflow. \n  \n     2.3.3. Controlling the overhead \n   In terms of efficiency, the current design with a separate reflection module is not ideal because it has an additional cost overhead. If we pass the full trajectory to the reflection module for each step, the token usage will be doubled, defeating the purpose of cost reduction. Therefore, controlling the overhead becomes our next topic. \n  \n   The first approach to consider is the choice of LLM in the reflection module. Since trajectory reduction is arguably easier than the original task of the agent, it is possible to significantly reduce the overhead by choosing a more cost-efficient LLM for the reflection module. In the previous case study illustrated in Fig. 2 , the trajectory is generated by the agent with Claude 4 Sonnet, and the reduction is performed by GPT-5 mini, which is 12 times cheaper than Claude 4 Sonnet, minimizing overhead concerns. We will discuss more about the choice of LLM in Section 4.2 . \n  \n   Figure 3. Design of the reflection module in AgentDiet.    To further reduce the overhead, we can reduce the amount of context for the LLM used in the reflection module. We propose a sliding-window approach, as shown in Fig. 3 . The yellow box in the figure refers to the reflection module: When the agent reaches step s s , the reflection module is now only allowed to reduce the content in step s \u2212 a s-a , given only a fixed amount of context (from step s \u2212 a \u2212 b s-a-b to step s s ). The module asks a cost-efficient LLM to reduce the content in Step s \u2212 a s-a by removing useless, redundant, and expired information given the context. a a and b b are hyperparameters that can be adjusted to balance the cost overhead and agent performance caused by trajectory reduction. Furthermore, we skip the trajectory reflection process if the length of step s \u2212 a s-a is within a token threshold \u03b8 \\theta , where the benefit would be small. Fig. 3 illustrates the approach where a = 2 a=2 , b = 1 b=1 and \u03b8 = 0 \\theta=0 . The choice of these hyperparameters will be discussed in Section 4.2 . \n  \n   From the perspective of efficiency, the token usage of each trajectory reduction step is capped at the length of a + 1 + b a+1+b steps as the trajectory grows. Also, since it may modify only a fixed recent step (Step s \u2212 a s-a ), entries in the KV Cache corresponding to previous steps will be preserved, minimizing the overhead introduced by invalidated KV Cache. From the perspective of agent performance, this design of the reflection module has the benefit of having no chance to destructively erase the most recent step or all steps at once, which prevents disastrous outcomes in occasional LLM failures. \n  \n       3. Algorithm \n   Algorithm 1  Integrating AgentDiet in a typical LLM agent    1: Input: Problem instruction ( I I ), Environment ( E E ) \n   2: Output: Result ( r r ), Modified environment ( E E ) \n   3: Constant: Number of steps before ( b b ) / after ( a a ) the target step in context, Step limit ( s max s_{\\text{max}} ), Length threshold ( \u03b8 \\theta ), LLM for the agent ( LLM agent \\text{LLM}_{\\text{agent}} ) / reflection ( LLM reflect \\text{LLM}_{\\text{reflect}} ) steps \n   4:  \n   5: T \u2190 MakeInitialPrompt \u200b ( I ) T\\leftarrow\\textsc{MakeInitialPrompt}(I)  \u22b3 \\triangleright Initiate the trajectory  \n   6: for each  s \u2208 [ 1 \u200b \u2026 \u200b s max ] s\\in[1\\ldots s_{\\text{max}}]  do  \n   7:  m assis \u2190 LLM agent \u200b ( T ) m_{\\text{assis}}\\leftarrow\\text{LLM}_{\\text{agent}}(T)  \u22b3 \\triangleright Perform agent step  \n   8:  if  IsTaskDone \u200b ( m assis ) \\textsc{IsTaskDone}(m_{\\text{assis}})  then  \n   9:  r \u2190 \"finished\" r\\leftarrow\\texttt{\"finished\"}  \n   10:  return r, E \n   11:  end  if  \n   12:  E , m tool \u2190 ExecTool \u200b ( E , m assis ) E,m_{\\text{tool}}\\leftarrow\\textsc{ExecTool}(E,m_{\\text{assis}})  \u22b3 \\triangleright Parse and execute the tool call in the environment  \n   13:  T \u2190 T + [ \u27e8 m assis , m tool \u27e9 ] T\\leftarrow T+[\\langle m_{\\text{assis}},m_{\\text{tool}}\\rangle]  \u22b3 \\triangleright Concatenate messages in this step into the trajectory  \n   14:  \n   15:  if  s \u2212 a > 0 s-a>0  then  \n   16:  l orig \u2190 Length \u200b ( Serialize \u200b ( [ T \u200b [ s \u2212 a ] ] ) ) l_{\\text{orig}}\\leftarrow\\textsc{Length}(\\textsc{Serialize}([T[s-a]]))  \n   17:  if  l orig > \u03b8 l_{\\text{orig}}>\\theta  then  \n   18:  c t x \u2190 Serialize ( T [ max ( 0 , s \u2212 a \u2212 b ) : s ] ) ctx\\leftarrow\\textsc{Serialize}(T[\\text{max}(0,s-a-b):s])  \n   19:  m reduced \u2190 LLM reflect \u200b ( c \u200b t \u200b x , s \u2212 a ) m_{\\text{reduced}}\\leftarrow\\text{LLM}_{\\text{reflect}}(ctx,s-a)  \u22b3 \\triangleright Perform reflection step  \n   20:  l reduced \u2190 Length \u200b ( m reduced ) l_{\\text{reduced}}\\leftarrow\\textsc{Length}(m_{\\text{reduced}})  \n   21:  \n   22:  if  l orig \u2212 l reduced > \u03b8 l_{\\text{orig}}-l_{\\text{reduced}}>\\theta  then  \u22b3 \\triangleright Apply reduction if benefit reaches the threshold  \n   23:  T \u200b [ s \u2212 a ] \u2190 m reduced T[s-a]\\leftarrow m_{\\text{reduced}}  \n   24:  end  if  \n   25:  end  if  \n   26:  end  if       \n   27: end  for  \n   28:  \n   29: r \u2190 \"interrupted\" r\\leftarrow\\texttt{\"interrupted\"}  \u22b3 \\triangleright Reaching the step limit  \n   30: return r, E \n  \n    In this section, we instantiate the design of AgentDiet introduced in Section 2.3 and Fig. 3 into a concrete algorithm as shown in Algorithm 1 . The orange box in Algorithm 1 shows how to integrate AgentDiet in a typical LLM agent. \n  \n   As a typical LLM agent, Algorithm 1 takes two arguments as input: the problem instruction I I , which contains a natural language description of the task to complete, and the environment E E , which the agent can interact with through tool calls. The output is the result r r that indicates whether the task is completed. The environment E E will be modified by the agent in place. The algorithm also contains several constants that are adjustable hyperparameters. \n  \n   The first part (lines 5 - 13 ) of this algorithm is directly based on the workflow of existing LLM agents. It first initiates the trajectory T T with only the initial system and user prompts constructed with I I . Then, it iteratively calls the LLM agent \\text{LLM}_{\\text{agent}} function to generate m assis m_{\\text{assis}} that contains the next action to take (in a tool call) based on T T . If the action is to finish the task, the agent stops; otherwise, the ExecTool function interacts with the environment E E based on the tool call and retrieves the result as m tool m_{\\text{tool}} . The messages generated in this step ( m assis m_{\\text{assis}} and m tool m_{\\text{tool}} ) are then concatenated into T T . \n  \n   Lines 15 - 26 highlighted with the orange box represent the new reflection module introduced in AgentDiet. As illustrated in Fig. 3 , the reflection module follows a sliding window that aims to reduce the length of step s \u2212 a s-a when the agent reaches step s s . The reflection module uses another language model, LLM reflect \\text{LLM}_{\\text{reflect}} , which can be more cost-efficient than LLM agent \\text{LLM}_{\\text{agent}} to reduce the overhead of trajectory reduction. This module is controlled by the hyperparameter \u03b8 \\theta , which is the minimum number of tokens of a beneficial reduction, and a , b a,b , which are the number of steps in the context given to LLM reflect \\text{LLM}_{\\text{reflect}} . It first serializes the target step into a string ( Serialize ) and calculates its tokenized length ( Length ) as l orig l_{\\text{orig}} at line 16 . If the length is too short ( l orig \u2264 \u03b8 l_{\\text{orig}}\\leq\\theta ), reflection is skipped for this step because the benefit may not exceed the overhead. Otherwise, it calculates the context ( c \u200b t \u200b x ctx ) and asks LLM reflect \\text{LLM}_{\\text{reflect}} to generate a reduced version of the target step based on c \u200b t \u200b x ctx . If the length reduction exceeds the token threshold \u03b8 \\theta , the target step in the trajectory T T will be replaced by the reduced version at line 23 . \n  \n     4. Implementation \n    4.1. Integration in Agents \n   AgentDiet is a general approach that can be applied simply to any LLM agent under the \u201cmodels using tools in a loop\u201d definition. For a typical agent system similar to Algorithm 1 , we can add a call to the reflection module (marked in the orange box) after each step in the loop. \n  \n   Following this procedure, we integrated AgentDiet in Trae Agent, which is open-source and ranked first on SWE-bench Verified when this research was performed (in July 2025). We did not further integrate and experiment AgentDiet in other LLM agents based on the observation that current agent systems are generally homogeneous, including a similar set of prompts and tools. For example, Trae Agent is equipped with four tools: bash to execute Bash commands, str_replace_editor to view or edit files, think to reason about the issue, and task_done to finish the task. This set of tools is semantically equivalent to tools in other systems, such as mini-SWE-agent (Yang et\u00a0al., 2024a ) and OpenHands (Wang et\u00a0al., 2024c ) . Therefore, the results with Trae Agent have a high chance of generalizing to a variety of similar agent systems. \n  \n   Note that there are variants of agent systems, such as ensembled systems that call the LLM agent multiple times and decide a final answer by majority voting, and multi-agent systems that simultaneously spawn multiple LLM agents with a communication mechanism. We can integrate AgentDiet in such systems by adding the reflection module in the LLM agent in an ensembled system, or in all LLM agents in a multi-agent system. As early research in the field, this paper does not focus on these applications. We leave the evaluation of such an application for future work. \n  \n     4.2. Hyperparameter Settings \n   In this subsection, we discuss the effect of different hyperparameter settings ( LLM reflect \\text{LLM}_{\\text{reflect}} , \u03b8 \\theta , a a , and b b , as shown in Algorithm 1 ) based on the Trae Agent integration. Note that the other two hyperparameters ( LLM agent \\text{LLM}_{\\text{agent}} and s max s_{\\text{max}} ) already exist in the original Trae Agent, so we omit the discussion and set them to a reasonable default value: LLM agent = \\text{LLM}_{\\text{agent}}= Claude 4 Sonnet, and s max = s_{\\text{max}}= 50. \n  \n   To set an ideal value for each of these hyperparameters, we first started with an initial setting ( LLM reflect = \\text{LLM}_{\\text{reflect}}= Gemini 2.5 Flash, \u03b8 = 500 \\theta=500 , a = 3 a=3 , and b = 1 b=1 ) and quantitatively compared the efficiency and agent performance with an experiment against variants, each with a different setting of one hyperparameter. Two variants (one with LLM reflect = \\text{LLM}_{\\text{reflect}}= GPT-5 mini, the other with a = 2 a=2 ) showed a better overall result than the initial one, so we iterated the hyperparameter setting to LLM reflect = \\text{LLM}_{\\text{reflect}}= GPT-5 mini, \u03b8 = 500 \\theta=500 , a = 2 a=2 , and b = 1 b=1 . We reran the experiment, and no variant showed a better overall result in this iteration. As a result, this hyperparameter setting is chosen as the ideal one. \n  \n   Below, we discuss the setup of this experiment and show the results in the second iteration. \n  \n    4.2.1. Benchmark \n   For this experiment, we randomly choose 100 instances from SWE-bench Verified (Jimenez et\u00a0al., 2024 ) as the benchmark. We do not use the full benchmark because the remaining instances will be used exclusively for further evaluation in Section 5 . \n  \n     4.2.2. Variants for LLM reflect \\text{LLM}_{\\text{reflect}}  \n   Table 1. Compared LLMs and baselines with their pricing information.      Pricing (US$ / M tokens)      Cached Input  Input  Output    LLMs  Claude 3.5 Haiku  0.080  0.80  4.0    Gemini 2.5 Flash  0.075  0.30  2.5    GPT-5 mini  0.030  0.25  2.0    DeepSeek v3 (0324)  0.070  0.27  1.1    Qwen 3 (2507)  0.070  0.27  1.1    Baselines  Original  N/A    LLMLingua-2  0.01    Random  N/A      This experiment considers five variants, covering a variety of recent cost-efficient LLMs from various vendors: Claude 3.5 Haiku, Gemini 2.5 Flash, GPT-5 mini, DeepSeek v3 (0324), and Qwen 3 (2507). The pricing information for these LLMs is listed in Table 1 , which will be used to calculate their computational cost. \n  \n   As a sanity check, we also added the following three additional baseline variants: \n  \n     \u2022   Original : The unmodified agent system that skips the reflection step (lines 15 - 26 ). \n  \n  \n   \u2022   LLMLingua-2  (Pan et\u00a0al., 2024 ) : A small model for prompt compression. Since this model runs on our own machine, we estimate its pricing (as US$0.01 / M tokens) based on a similar BERT model with 600M parameters. \n  \n  \n   \u2022   Random : A baseline that randomly deletes 75% processed tokens. \n  \n  \n   \u2022   Delete : A baseline that deletes all processed tokens. \n  \n  \n   \n   Note that LLM reflect \\text{LLM}_{\\text{reflect}} processes a step only when its length is greater than \u03b8 \\theta tokens (line 17 in Algorithm 1 ). As a result, all baseline variants (even the Delete baseline) will keep all tokens in a short step as-is. \n  \n     4.2.3. Variants for \u03b8 \\theta , a a , and b b  \n   The variants considered for these hyperparameters are shown in the list below: \n  \n     \u2022   \u03b8 = \\theta= 0, 250, 500, 1000, 2000. \n  \n  \n   \u2022   a = a= 0, 1, 2, 3. \n  \n  \n   \u2022   b = b= 0, 1, 2. \n  \n  \n   \n   Note that we limit the choice of a a and b b to an upper bound ( a \u2264 3 , b \u2264 2 a\\leq 3,b\\leq 2 ) because a higher value would strictly cause more cost overhead with the opportunity of better agent performance. Since the variants of a = 2 a=2 and b = 1 b=1 already have a decent performance, as will be shown in the results, we estimate that there would be little benefit in further increasing a a or b b . \n  \n     4.2.4. Metrics \n   We use the following metrics to measure the amount of efficiency improvement for each variant: \n  \n     \u2022   Keep% : The average ratio of trajectory tokens kept by the LLM in the reflection module, formally \u2211 l reduced / \u2211 l orig \\sum l_{\\text{reduced}}\\>/\\>\\sum l_{\\text{orig}} in Algorithm 1 . \n  \n  \n   \u2022   I and O : Normalized accumulated usage of input (\u201cprompt\u201d) / output (\u201ccompletion\u201d) tokens. We normalize all numbers in these two rows so that the Original baseline has I = 1 I=1 . \n  \n  \n   \u2022   $ and $+ : Normalized LLM cost for agent steps ($) and reflection steps ($+). This value is calculated based on the pricing for the LLMs involved. The cost covers both input tokens (considering the discount for tokens in KV Cache) and output tokens. We normalize all numbers in these two rows so that the Original baseline has $ = 1 \\mathdollar=1 . We separately report the cost for agent steps and reflection steps to help readers better understand the overhead caused by the reflection module. \n  \n  \n   \n   Additionally, we use the following metrics to measure the impact on the agent\u2019s performance: \n  \n     \u2022   Pass% : The ratio of successfully resolved instances in the benchmark. \n  \n  \n   \u2022   Step and PStep : The average number of total agent steps for all instances (Step) and for only successfully resolved instances (PStep). Lower is better, because an increase in steps indicates that some information is incorrectly reduced, disturbing the agent and requiring it to recover the information with extra steps. \n  \n  \n   \n     4.2.5. Results for LLM reflect \\text{LLM}_{\\text{reflect}}  \n   Table 2. Results of different LLMs or baselines as LLM reflect \\text{LLM}_{\\text{reflect}} .     Claude  Gemini  GPT  DeepSeek  Qwen  Orig.  Lingua  Random  Delete    Keep%  14.4  21.9  28.6  23.7  29.0   22.5  25.0  0    I  0.473  0.553  0.586  0.606  0.722  1.000  0.603  0.642  0.428    O  0.012  0.012  0.012  0.012  0.012  0.012  0.013  0.013  0.013    $  0.652  0.698  0.707  0.733  0.818  1.000  0.753  0.794  0.655    $+  0.148  0.078  0.077  0.052  0.065   0.000      Pass%  60  63  65  64  62  65  61  64  58    Step  42.39  40.95  38.90  40.41  41.10  39.74  43.89  44.95  45.43    Pstep  40.27  39.46  37.34  38.47  39.69  38.29  42.21  43.66  43.69      The LLM to use for the reflection step is an important hyperparameter in AgentDiet, since it should be both cost-efficient and capable enough to identify the waste. \n  \n   The results are illustrated in Table 2 . The first row (Keep%) shows that all five LLMs can remove most of the content in the trajectory steps where the length is at least \u03b8 = 500 \\theta=500 tokens. However, different LLMs have different characteristics, keeping 14.4% \u223c \\sim 29.0% tokens in the processed steps. As a result, the accumulated input tokens (I) drop to 47.3% \u223c \\sim 72.2%, where the output tokens (O) remain close to the Original baseline. The agent cost ($) drops to 65.2% \u223c \\sim 81.8% of Original. The \u201c$+\u201d row indicates that LLM reflect \\text{LLM}_{\\text{reflect}} introduces an overhead cost of 5.2% \u223c \\sim 14.8%, depending on the model used. \n  \n   The last three rows measure the performance impact of the trajectory reduction on the SWE-Bench Verified instances. We can see that while the numbers in Pass% vary between LLMs and baselines, all of them reach a Pass% of at least 58% (for the Delete baseline), which is close to the 65% achieved by the Original agent. This is due to the default choice of other hyperparameters (especially a = 2 a=2 and \u03b8 = 500 \\theta=500 ), which ensures that LLM reflect \\text{LLM}_{\\text{reflect}} can only manipulate steps of excessive length after a delay of 2 steps. Therefore, the disruption to LLM agent \\text{LLM}_{\\text{agent}} is limited, and LLM can recover the disrupted information by performing on its own through tool calls, leading to an increase in \u201cStep\u201d and \u201cPStep\u201d. We can see that the increase is most severe for the Delete baseline that maximized the disturbance to the trajectory. \n  \n   Based on the results, GPT-5 mini maintains the same Pass% as Original, and is also the only model with a decrease in Step and PStep, showing that it does not harm the agent\u2019s performance while saving significant token costs. Therefore, GPT-5 mini is chosen as the final LLM reflect \\text{LLM}_{\\text{reflect}} in AgentDiet. \n  \n     4.2.6. Results for \u03b8 \\theta , a a , and b b  \n   Table 3. Results of different hyperparameter settings ( \u03b8 \\theta , a a , and b b ).       \u03b8 \\theta     Orig.  0  250  500  1000  2000    Keep%   32.6  32.0  28.6  24.3  16.2    I  1.000  0.547  0.587  0.586  0.662  0.728    O  0.012  0.011  0.011  0.012  0.012  0.012    $  1.000  0.669  0.700  0.707  0.765  0.816    $+   0.118  0.100  0.077  0.040  0.017    Pass%  65  62  65  65  60  58    Step  39.74  38.57  39.68  38.90  39.70  40.22    Pstep  38.29  37.40  38.09  37.34  38.27  38.91    \n  \n      a a  b b     0  1  2  3  0  1  2    Keep%  31.7  31.0  28.6  31.3  31.5  28.6  34.5    I  0.727  0.569  0.586  0.624  0.644  0.586  0.719    O  0.013  0.011  0.012  0.012  0.012  0.012  0.011    $  0.843  0.688  0.707  0.736  0.749  0.707  0.790    $+  0.081  0.067  0.077  0.078  0.075  0.077  0.078    Pass%  59  62  65  66  64  65  65    Step  44.94  39.13  38.90  40.25  40.31  38.90  39.32    PStep  43.59  37.73  37.34  40.06  39.45  37.34  37.49    \n  \n    The results for the remaining hyperparameters are illustrated in Table 3 . For \u03b8 \\theta , a higher threshold value results in more input tokens for the agent (I), but fewer tokens for reflection overhead. \u03b8 = 500 \\theta=500 is a balance between token saving and reflection overhead. In terms of agent performance, \u03b8 = 500 \\theta=500 also shows optimal performance close to Original. \n  \n   For a a and b b , a higher value delays the trajectory reduction and gives the reflection module more context, hence resulting in better performance but worse efficiency. a = 2 , b = 1 a=2,b=1 is the minimum choice of a a and b b that has a negligible performance harm, while still improving the efficiency by more than 22% as shown in the $ and $+ rows. \n  \n       5. Evaluation \n   In this section, we empirically evaluate AgentDiet with the following research questions. \n  \n     RQ1.   Efficiency Improvement: Can AgentDiet improve the efficiency of LLM agents by reducing the length of the trajectory? \n  \n  \n   RQ2.   Performance Impact: Does AgentDiet harm the performance of LLM agents? \n  \n  \n   RQ3.   Generalization: How do the results generalize to different benchmarks and LLMs? \n  \n  \n   \n    5.1. Experimental Setup \n    5.1.1. Experiment subjects \n   We integrated AgentDiet in Trae Agent (Gao et\u00a0al., 2025 ) , as discussed in Section 4 . To learn more about the generalization of our approach (RQ3), we evaluated the Trae Agent integration with two recent LLMs (i.e., LLM agent \\text{LLM}_{\\text{agent}} ) showing decent agent capability: Claude 4 Sonnet and Gemini 2.5 Pro. For each LLM, we compare the results of AgentDiet over the Original baseline, where the reflection step is skipped. \n  \n     5.1.2. Metrics \n   We used the same set of metrics as previously described in Section 4.2.4 , which contains five metrics for efficiency (Keep%, I, O, $, and $+), and three metrics for performance impact (Pass%, Step, and PStep). In RQ1, we can infer that AgentDiet improves efficiency if Keep% is low and I/O/$ decreases over Original; In RQ2, we can infer that AgentDiet does not harm the agent\u2019s performance if Pass% does not decrease and Step/PStep does not increase over Original. \n  \n     5.1.3. Benchmarks \n   We used two benchmarks for the evaluation: \n  \n     \u2022   SWE-bench Verified  (Jimenez et\u00a0al., 2024 ) : This benchmark contains 500 human-verified software engineering tasks in Python based on GitHub issues in the real world. Since Section 4.2 used 100 instances in this benchmark for hyperparameter selection, we excluded them and randomly selected 200 instances from the remaining 400 instances for the experiment in this section. Similar to the validation/test split in standard machine learning practice, this experimental setup addresses the overfitting threat of hyperparameters. The list of selected instances is available in the artifact. \n  \n  \n   \u2022   Multi-SWE-bench Flash  (Zan et\u00a0al., 2025 ) : Similar to SWE-bench Verified, this benchmark contains 300 instances based on GitHub issues. However, this benchmark covers tasks in seven other programming languages, containing 45 Rust instances, 45 TypeScript instances, 45 JavaScript instances, 40 Java instances, 45 Go instances, 40 C instances, and 40 C++ instances. These tasks are generally harder than SWE-bench Verified, and often require the agent to figure out how to build the project, whereas SWE-bench Verified gives the agent a fully working environment in the beginning. \n  \n  \n   \n     5.1.4. Hyperparameters \n   For the hyperparameters related to AgentDiet, we follow the experiment results in Section 4.2 , setting a = 2 a=2 , b = 1 b=1 , \u03b8 = 500 \\theta=500 and LLM reflect = GPT-5 mini \\text{LLM}_{\\text{reflect}}=\\text{GPT-5 mini} . \n  \n   For the step limit ( s max s_{\\text{max}} ), we set it to 50 for the SWE-bench Verified benchmark as the default value in Trae Agent. s max s_{\\text{max}} is increased to 100 for Multi-SWE-bench Flash because this benchmark requires more steps to complete, as will be shown from Step/PStep metrics in the experiment results. \n  \n      5.2. Results \n   Table 4. Efficiency improvement and performance impact of AgentDiet.    Benchmark  SWE-bench Verified  Multi-SWE-bench Flash    LLM agent \\text{LLM}_{\\text{agent}}  Claude 4 Sonnet  Gemini 2.5 Pro  Claude 4 Sonnet  Gemini 2.5 Pro    Approach  Orig.  AgentDiet  Orig.  AgentDiet  Orig.  AgentDiet  Orig.  AgentDiet    Keep%   30.8   22.6   30.2   25.7    I  1.000  0.601  1.000  0.591  1.000  0.596  1.000  0.403    O  0.012  0.011  0.007  0.006  0.006  0.006  0.007  0.009    $  1.000  0.714  1.000  0.623  1.000  0.676  1.000  0.559    $+   0.074   0.118   0.055   0.082    Pass%  64.5  66.5  50.5  52.0  40.0  39.0  21.7  22.7    Step  39.62  39.95  37.98  37.44  70.37  70.45  57.20  43.90    PStep  37.75  38.21  32.59  31.72  62.08  60.77  37.86  29.75      Table 4 contains the main results of the experiment, and we will discuss them with respect to the three research questions, i.e., efficiency improvement, performance impact, and generalization. \n  \n    5.2.1. RQ1. Efficiency Improvement \n   We can understand how the trajectory reduction by AgentDiet ultimately improves efficiency through various metrics reported in Table 4 . First, the reflection module removes 69.2% \u223c \\sim 77.4% content it processed (1 - Keep%). This results in a reduction in the accumulated input tokens (1 - I) at 39.9% \u223c \\sim 59.7%. This reduction is less than the percentage of deleted processed tokens because the reflection module only processes steps longer than the token threshold ( \u03b8 = 500 \\theta=500 ) and only applies the reduction after several ( a = 2 a=2 ) steps. The reduced input tokens then lead to a reduced computational cost of the agent ($) at 28.6% \u223c \\sim 44.1%. This reduction is again smaller due to costs related to output tokens and invalidated KV Caches, but still significant enough. After we further consider the computational overhead of the reflection module itself ($+), the final cost reduction becomes 21.1% \u223c \\sim 35.9%. \n  \n   Note that the numbers in \u201c$\u201d and \u201c$+\u201d rows are normalized to make the relative comparison with Original easier. If we convert the numbers back to the format of average US$ per instance, the Original baseline costs $0.535, $0.385, $1.277, and $0.701, respectively, for the SbV+Claude, SbV+Gemini, MSbF+Claude, and MSbF+Gemini columns. AgentDiet decreased the cost to $0.422, $0.285, $0.933, and $0.449, respectively. Considering that each instance only represents a single minor task of a software engineer (with a typical code modification within one or a few functions), the total savings among active users of a popular AI product can be a large number. \n  \n     Finding 1. AgentDiet significantly removes waste in the trajectory, leading to a reduction in input tokens by 39.9% \u223c \\sim 59.7%, or a reduction in the final computational cost by 21.1% \u223c \\sim 35.9%, compared to the Original baseline.   \n     5.2.2. RQ2. Performance Impact \n   From the \u201cPass%\u201d row in Table 4 , we can compare the numbers between Original and AgentDiet to learn its impact to the agent\u2019s performance. On two benchmarks and two LLMs, the performance of AgentDiet is comparable (-1.0% \u223c \\sim +2.0%) to Original. The result shows that AgentDiet does not harm the agent\u2019s performance while improving efficiency, which contradicts the common belief of \u201ctest-time compute\u201d (Beeching et\u00a0al., 2025 ) that there is a trade-off between token efficiency and model performance. As discussed in Section 2.1 , a possible explanation is that the performance of LLM degrades as the length of the context increases (Li et\u00a0al., 2025 ) or when the context is of low quality (Laban et\u00a0al., 2025 ) . Therefore, the removal of waste information from the agent trajectory will cause less degradation to the model\u2019s performance. \n  \n   Figure 4. Distribution of the Step metric on Multi-SWE-bench Flash with Gemini 2.5 Pro between Original (left) and AgentDiet (right).    Additionally, from the \u201cStep\u201d and \u201cPStep\u201d rows, we can learn that AgentDiet does not cause the steps required to resolve a task to increase. This is an additional indicator confirming that AgentDiet does not disturb the agent. An interesting case occurs in the last column, where AgentDiet significantly reduced the average steps from 57.20 to 43.90 when Gemini 2.5 Pro is working on the Multi-SWE-bench Flash benchmark. We found the reason through inspecting the trajectories: Gemini 2.5 Pro shows increasingly abnormal behavior when the context is long, often resulting in repeating the same tool call until the step limit is reached. Therefore, AgentDiet reduces this probability by reducing the length of the trajectory by half. Fig. 4 draws the histogram of the step distribution in the experiment with Gemini 2.5 Pro + Multi-SWE-bench Flash, where AgentDiet reduces the number of instances that reach the step limit (at 100 steps) from 66 to 26. As a result, the average steps are significantly reduced. \n  \n     Finding 2. The ratio of successfully resolved instances for AgentDiet is comparable (-1.0% \u223c \\sim +2.0%) to the Original baseline. AgentDiet also does not cause the agent to take more steps to resolve a task. Therefore, trajectory reduction does not harm the agent\u2019s performance.   \n     5.2.3. RQ3. Generalization \n   The generalization of AgentDiet across benchmarks and LLMs can be confirmed by comparing the columns in Table 4 , where AgentDiet steadily improves efficiency and maintains the same performance in two benchmarks (SWE-bench Verified and Multi-SWE-bench Flash) and two LLMs (Claude 4 Sonnet and Gemini 2.5 Pro). \n  \n   Table 5. Efficiency improvement and performance impact of AgentDiet on Multi-SWE-bench Flash instances categorized by programming languages.     Lang  Rust  TypeScript  JavaScript  Java  Go  C  C++     #Inst.  45  45  45  40  45  40  40         Claude 4 Sonnet \n  \n      Keep%  28.1  29.9  29.8  31.7  34.0  24.7  32.7    I  0.603  0.560  0.649  0.637  0.615  0.543  0.602    O  0.006  0.006  0.010  0.007  0.006  0.005  0.006    $  0.680  0.635  0.806  0.723  0.646  0.620  0.679    $+  0.061  0.049  0.085  0.057  0.049  0.043  0.049     \u0394 \\Delta Pass  -2  +1  +1  +1  -3  -1  0     \u0394 \\Delta Step  -0.27  -1.11  +3.11  -1.32  +0.13  +0.40  -0.53     \u0394 \\Delta PStep  -3.67  +0.67  +1.24  +2.30  -4.84  -2.00  -1.38         Gemini 2.5 Pro \n  \n      Keep%  22.3  25.5  34.1  26.1  28.1  18.3  24.4    I  0.310  0.525  0.459  0.330  0.656  0.443  0.298    O  0.007  0.015  0.\n[truncated]",
  "title": "Improving the Efficiency of LLM Agent Systems through Trajectory Reduction",
  "content_type": "text/html; charset=utf-8"
}